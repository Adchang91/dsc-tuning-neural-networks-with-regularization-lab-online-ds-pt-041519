{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Tuning Neural Networks with Regularization - Lab"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Introduction\n", "\n", "Recall from the last lab that you had a training accuracy close to 90% and a test set accuracy close to 76%.\n", "\n", "As with your previous machine learning work, you should be asking a couple of questions:\n", "- Is there a high bias? yes/no\n", "- Is there a high variance? yes/no \n", "\n", "In this lab, you'll use the a train-validate-test partition to get better insights of how to tune neural networks using regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, you'll see how to include a validation set. From there, you'll define and compile the model like before. However, this time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test set but also the validation set.\n", "\n", "## Objectives\n", "\n", "You will be able to:\n", "\n", "* Construct and run a basic model in Keras\n", "* Construct a validation set and explain potential benefits\n", "* Apply L1 and L2 regularization\n", "* Apply dropout regularization\n", "* Observe and comment on the effect of using more data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Import the libraries\n", "\n", "As usual, start by importing some of the packages and modules that you intend to use. The first thing you'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Your code here; import some packages/modules you plan to use"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Load the Data\n", "\n", "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Your code here; load and preview the dataset"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Preprocessing Overview\n", "\n", "Before you begin to practice some of your new tools regarding regularization and optimization, let's practice munging some data as you did in the previous section with bank complaints. Recall some techniques:\n", "\n", "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n", "* One-hot encoding your complaint text\n", "* Transforming your category labels\n", "* Train - test split"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Preprocessing: Generate a Random Sample\n", "\n", "Since you have quite a bit of data and training networks takes a substantial amount of time and resources, downsample in order to test your initial pipeline. Going forward, these can be interesting areas of investigation: how does your models performance change as you increase (or decrease) the size of your dataset?  \n", "\n", "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Your code here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Preprocessing: One-hot Encoding of the Complaints\n", "\n", "As before, you need to do some preprocessing and data manipulationg before building the neural network. \n", "\n", "Keep the 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n", "#Only keep the 2000 most common words."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Preprocessing: Encoding the Products\n", "\n", "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n", "  \n", "> **Note**: This is similar to your previous work with dummy variables. Each of the various product categories will be its own column, and each observation will be a row. In turn, each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Your code here; transform the product labels to numerical values\n", "#Then transform these integer values into a matrix of binary flags"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Train-test Split\n", "\n", "Now onto the ever familiar train-test split! \n", "Below, perform an appropriate train test split.\n", "> Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Yyour code here\n", "X_train = \n", "X_test = \n", "y_train = \n", "y_test = "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Running the model using a validation set."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Creating the Validation Set"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In the lecture, you saw that in deep learning, you generally set aside a validation set, which is then used during hyperparameter tuning. Afterwards, when you have decided upon a final model, the test can then be used to define the final model perforance. \n", "\n", "In this example, take the first 1000 cases out of the training set to create a validation set. You should do this for both `train` and `label_train`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Just run this block of code \n", "random.seed(123)\n", "val = X_train[:1000]\n", "train_final = X_train[1000:]\n", "label_val = y_train[:1000]\n", "label_train_final = y_train[1000:]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Creating the Model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Rebuild a fully connected (Dense) layer network with relu activations in Keras."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Recall that you used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because you are dealing with a multiclass problem (classifying the complaints into 7 classes), use a softmax classifyer in order to output 7 class probabilities per case. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Your code here; build a neural network using Keras as described above.\n", "model = "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Compiling the Model\n", "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, include the argument `validation_data` and assign it `(val, label_val)`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Your code here"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Training the Model\n", "\n", "Ok, now for the resource intensive part: time to train your model! Note that this is where you also introduce the validation data to the model."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#Code provided; note the extra validation parameter passed.\n", "model_val = model.fit(train_final,\n", "                    label_train_final,\n", "                    epochs=120,\n", "                    batch_size=256,\n", "                    validation_data=(val, label_val))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Retrieving Performance Results: the `history` dictionary\n", "\n", "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model_val_dict = model_val.history\n", "model_val_dict.keys()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results_train = model.evaluate(train_final, label_train_final)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results_test = model.evaluate(X_test, y_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results_train"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results_test"]}, {"cell_type": "markdown", "metadata": {}, "source": ["To interpret these results, run the cell below:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["model.metrics_names"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The first element of the list returned by `model.evaluate` is the loss, and the second is the accuracy score. \n", "\n", "Note that the result you obtained here isn't exactly the same as before. This is because the training set is slightly different! You removed 1000 instances for validation!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Plotting the Results"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Plot the loss function versus the number of epochs. Be sure to include the training and the validation loss in the same plot. Then, create a second plot comparing training and validation accuracy to the number of epochs."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.clf()\n", "\n", "import matplotlib.pyplot as plt\n", "loss_values = model_val_dict['loss']\n", "val_loss_values = model_val_dict['val_loss']\n", "\n", "epochs = range(1, len(loss_values) + 1)\n", "plt.plot(epochs, loss_values, 'g', label='Training loss')\n", "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n", "\n", "plt.title('Training & validation loss')\n", "plt.xlabel('Epochs')\n", "plt.ylabel('Loss')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.clf()\n", "\n", "acc_values = model_val_dict['acc'] \n", "val_acc_values = model_val_dict['val_acc']\n", "\n", "plt.plot(epochs, acc_values, 'r', label='Training acc')\n", "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n", "plt.title('Training & validation accuracy')\n", "plt.xlabel('Epochs')\n", "plt.ylabel('Loss')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Notice an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a limit around the 60th epoch. This means that you're probably **overfitting** the model to the training data when you train for many epochs past this dropoff point of around 40 epochs. Luckily, you learned how to tackle overfitting in the previous lecture! Since it seems clear that you are training too long, include early stopping at the 60th epoch first."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Early Stopping\n", "\n", "Below, observe how to update the model to include an earlier cutoff point:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["random.seed(123)\n", "model = models.Sequential()\n", "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n", "model.add(layers.Dense(25, activation='relu'))\n", "model.add(layers.Dense(7, activation='softmax'))\n", "\n", "model.compile(optimizer='SGD',\n", "              loss='categorical_crossentropy',\n", "              metrics=['accuracy'])\n", "\n", "final_model = model.fit(train_final,\n", "                    label_train_final,\n", "                    epochs=60,\n", "                    batch_size=256,\n", "                    validation_data=(val, label_val))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, you can use the test set to make label predictions"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results_train = model.evaluate(train_final, label_train_final)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results_test = model.evaluate(X_test, y_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results_train"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results_test"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We've significantly reduced the variance, so this is already pretty good! your test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs model you originally fit.\n", "\n", "Now, take a look at how regularization techniques can further improve your model performance."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## L2 Regularization"]}, {"cell_type": "markdown", "metadata": {}, "source": ["First, take a look at L2 regularization. Keras makes L2 regularization easy. Simply add the `kernel_regularizer=keras.regularizers.l2(lambda_coeff)` parameter to any model layer. The `lambda_coeff` parameter determines the strength of the regularization you wish to perform."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from keras import regularizers\n", "random.seed(123)\n", "model = models.Sequential()\n", "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n", "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n", "model.add(layers.Dense(7, activation='softmax'))\n", "\n", "model.compile(optimizer='SGD',\n", "              loss='categorical_crossentropy',\n", "              metrics=['accuracy'])\n", "\n", "L2_model = model.fit(train_final,\n", "                    label_train_final,\n", "                    epochs=120,\n", "                    batch_size=256,\n", "                    validation_data=(val, label_val))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["L2_model_dict = L2_model.history\n", "L2_model_dict.keys()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["plt.clf()\n", "\n", "acc_values = L2_model_dict['acc'] \n", "val_acc_values = L2_model_dict['val_acc']\n", "model_acc = model_val_dict['acc']\n", "model_val_acc = model_val_dict['val_acc']\n", "\n", "epochs = range(1, len(acc_values) + 1)\n", "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n", "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n", "plt.plot(epochs, model_acc, 'r', label='Training acc')\n", "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n", "plt.title('Training & validation accuracy L2 vs regular')\n", "plt.xlabel('Epochs')\n", "plt.ylabel('Loss')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The results of L2 regularization are quite disappointing here. Notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## L1 Regularization"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Have a look at L1 regularization. Will this work better?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["random.seed(123)\n", "model = models.Sequential()\n", "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n", "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n", "model.add(layers.Dense(7, activation='softmax'))\n", "\n", "model.compile(optimizer='SGD',\n", "              loss='categorical_crossentropy',\n", "              metrics=['accuracy'])\n", "\n", "L1_model = model.fit(train_final,\n", "                    label_train_final,\n", "                    epochs=120,\n", "                    batch_size=256,\n", "                    validation_data=(val, label_val))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["L1_model_dict = L1_model.history\n", "plt.clf()\n", "\n", "acc_values = L1_model_dict['acc'] \n", "val_acc_values = L1_model_dict['val_acc']\n", "\n", "epochs = range(1, len(acc_values) + 1)\n", "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n", "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n", "plt.title('Training & validation accuracy with L1 regularization')\n", "plt.xlabel('Epochs')\n", "plt.ylabel('Loss')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Notice how the training and validation accuracy don't diverge as much as before. Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like you can still improve the model by training much longer."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u23f0 This cell may take several minutes to run\n", "random.seed(123)\n", "model = models.Sequential()\n", "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n", "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n", "model.add(layers.Dense(7, activation='softmax'))\n", "\n", "model.compile(optimizer='SGD',\n", "              loss='categorical_crossentropy',\n", "              metrics=['accuracy'])\n", "\n", "L1_model = model.fit(train_final,\n", "                    label_train_final,\n", "                    epochs=1000,\n", "                    batch_size=256,\n", "                    validation_data=(val, label_val))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["L1_model_dict = L1_model.history\n", "plt.clf()\n", "\n", "acc_values = L1_model_dict['acc'] \n", "val_acc_values = L1_model_dict['val_acc']\n", "\n", "epochs = range(1, len(acc_values) + 1)\n", "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n", "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n", "plt.title('Training & validation accuracy L2 vs regular')\n", "plt.xlabel('Epochs')\n", "plt.ylabel('Loss')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results_train = model.evaluate(train_final, label_train_final)\n", "\n", "results_test = model.evaluate(X_test, y_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results_train"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results_test"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This is about the best result you've achieved so far, but you were training for quite a while! Next, experiment with dropout regularization to see if it offers any advantages."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Dropout Regularization"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u23f0 This cell may take about a minute to run\n", "random.seed(123)\n", "model = models.Sequential()\n", "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n", "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n", "model.add(layers.Dropout(0.3))\n", "model.add(layers.Dense(25, activation='relu'))\n", "model.add(layers.Dropout(0.3))\n", "model.add(layers.Dense(7, activation='softmax'))\n", "\n", "model.compile(optimizer='SGD',\n", "              loss='categorical_crossentropy',\n", "              metrics=['accuracy'])\n", "\n", "dropout_model = model.fit(train_final,\n", "                    label_train_final,\n", "                    epochs=200,\n", "                    batch_size=256,\n", "                    validation_data=(val, label_val))"]}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true}, "outputs": [], "source": ["results_train = model.evaluate(train_final, label_train_final)\n", "results_test = model.evaluate(X_test, y_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results_train"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results_test"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can see here that the validation performance has improved again! The variance did become higher again compared to L1-regularization."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Bigger Data?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In the lecture, one of the solutions to high variance was just getting more data. You actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple your data set, and see what happens. Note that you are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('Bank_complaints.csv')\n", "random.seed(123)\n", "df = df.sample(40000)\n", "df.index = range(40000)\n", "product = df[\"Product\"]\n", "complaints = df[\"Consumer complaint narrative\"]\n", "\n", "#one-hot encoding of the complaints\n", "tokenizer = Tokenizer(num_words=2000)\n", "tokenizer.fit_on_texts(complaints)\n", "sequences = tokenizer.texts_to_sequences(complaints)\n", "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n", "word_index = tokenizer.word_index\n", "np.shape(one_hot_results)\n", "\n", "#one-hot encoding of products\n", "le = preprocessing.LabelEncoder()\n", "le.fit(product)\n", "list(le.classes_)\n", "product_cat = le.transform(product) \n", "product_onehot = to_categorical(product_cat)\n", "\n", "# train test split\n", "test_index = random.sample(range(1,40000), 4000)\n", "test = one_hot_results[test_index]\n", "train = np.delete(one_hot_results, test_index, 0)\n", "label_test = product_onehot[test_index]\n", "label_train = np.delete(product_onehot, test_index, 0)\n", "\n", "#Validation set\n", "random.seed(123)\n", "val = train[:3000]\n", "train_final = train[3000:]\n", "label_val = label_train[:3000]\n", "label_train_final = label_train[3000:]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u23f0 This cell may take several minutes to run\n", "random.seed(123)\n", "model = models.Sequential()\n", "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n", "model.add(layers.Dense(25, activation='relu'))\n", "model.add(layers.Dense(7, activation='softmax'))\n", "\n", "model.compile(optimizer='SGD',\n", "              loss='categorical_crossentropy',\n", "              metrics=['accuracy'])\n", "\n", "moredata_model = model.fit(train_final,\n", "                    label_train_final,\n", "                    epochs=120,\n", "                    batch_size=256,\n", "                    validation_data=(val, label_val))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results_train = model.evaluate(train_final, label_train_final)\n", "results_test = model.evaluate(test, label_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results_train"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results_test"]}, {"cell_type": "markdown", "metadata": {}, "source": ["With the same amount of epochs, you were able to get a fairly similar validation accuracy of 89.67 (compared to 88.45 in obtained in the first model in this lab). Your test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Additional Resources\n", "\n", "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n", "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n", "* https://catalog.data.gov/dataset/consumer-complaint-database"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Summary  \n", "\n", "In this lesson, you not only built an initial deep-learning model, you then used a validation set to tune your model using various types of regularization."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.6"}, "toc": {"base_numbering": 1, "nav_menu": {}, "number_sections": true, "sideBar": true, "skip_h1_title": false, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}}, "nbformat": 4, "nbformat_minor": 2}