{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Neural Networks with Regularization - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that you had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with your previous machine learning work, you should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no \n",
    "\n",
    "In this lab, you'll use the a train-validate-test partition to get better insights of how to tune neural networks using regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, you'll see how to include a validation set. From there, you'll define and compile the model like before. However, this time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test set but also the validation set.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Apply dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing you'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; import some packages/modules you plan to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "#Your code here; import some packages/modules you plan to use\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; load and preview the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "#Your code here; load and preview the dataset\n",
    "\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before you begin to practice some of your new tools regarding regularization and optimization, let's practice munging some data as you did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding your complaint text\n",
    "* Transforming your category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since you have quite a bit of data and training networks takes a substantial amount of time and resources, downsample in order to test your initial pipeline. Going forward, these can be interesting areas of investigation: how does your models performance change as you increase (or decrease) the size of your dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "#Your code here\n",
    "random.seed(123)\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, you need to do some preprocessing and data manipulationg before building the neural network. \n",
    "\n",
    "Keep the 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "> **Note**: This is similar to your previous work with dummy variables. Each of the various product categories will be its own column, and each observation will be a row. In turn, each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "#Then transform these integer values into a matrix of binary flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "#Your code here; transform the product labels to numerical values\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product) \n",
    "\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! \n",
    "Below, perform an appropriate train test split.\n",
    "> Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yyour code here\n",
    "X_train = \n",
    "X_test = \n",
    "y_train = \n",
    "y_test = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, product_onehot, test_size=1500, random_state=42)\n",
    "\n",
    "#Alternative custom script:\n",
    "# random.seed(123)\n",
    "# test_index = random.sample(range(1,10000), 1500)\n",
    "# test = one_hot_results[test_index]\n",
    "# train = np.delete(one_hot_results, test_index, 0)\n",
    "# label_test = product_onehot[test_index]\n",
    "# label_train = np.delete(product_onehot, test_index, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, you saw that in deep learning, you generally set aside a validation set, which is then used during hyperparameter tuning. Afterwards, when you have decided upon a final model, the test can then be used to define the final model perforance. \n",
    "\n",
    "In this example, take the first 1000 cases out of the training set to create a validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that you used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because you are dealing with a multiclass problem (classifying the complaints into 7 classes), use a softmax classifyer in order to output 7 class probabilities per case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "#Your code here; build a neural network using Keras as described above.\n",
    "random.seed(123)\n",
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "#Your code here\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train your model! Note that this is where you also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9514 - acc: 0.1745 - val_loss: 1.9359 - val_acc: 0.1830\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.9169 - acc: 0.2111 - val_loss: 1.9126 - val_acc: 0.2050\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.8930 - acc: 0.2324 - val_loss: 1.8926 - val_acc: 0.2180\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8708 - acc: 0.2373 - val_loss: 1.8714 - val_acc: 0.2310\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8476 - acc: 0.2524 - val_loss: 1.8479 - val_acc: 0.2490\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8218 - acc: 0.2756 - val_loss: 1.8210 - val_acc: 0.2620\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.7923 - acc: 0.2989 - val_loss: 1.7903 - val_acc: 0.2910\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.7596 - acc: 0.3248 - val_loss: 1.7559 - val_acc: 0.3230\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.7238 - acc: 0.3541 - val_loss: 1.7183 - val_acc: 0.3620\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.6848 - acc: 0.3815 - val_loss: 1.6776 - val_acc: 0.3870\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.6430 - acc: 0.4085 - val_loss: 1.6340 - val_acc: 0.4240\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 1.5994 - acc: 0.4385 - val_loss: 1.5889 - val_acc: 0.4580\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5538 - acc: 0.4657 - val_loss: 1.5431 - val_acc: 0.4690\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5073 - acc: 0.4891 - val_loss: 1.4943 - val_acc: 0.5080\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4601 - acc: 0.5173 - val_loss: 1.4468 - val_acc: 0.5250\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 1.4131 - acc: 0.5435 - val_loss: 1.3994 - val_acc: 0.5480\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 1.3661 - acc: 0.5649 - val_loss: 1.3508 - val_acc: 0.5810\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 1.3202 - acc: 0.5924 - val_loss: 1.3052 - val_acc: 0.6070\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.2752 - acc: 0.6124 - val_loss: 1.2608 - val_acc: 0.6250\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2312 - acc: 0.6311 - val_loss: 1.2171 - val_acc: 0.6380\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.1889 - acc: 0.6415 - val_loss: 1.1770 - val_acc: 0.6490\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1487 - acc: 0.6516 - val_loss: 1.1366 - val_acc: 0.6560\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.1101 - acc: 0.6641 - val_loss: 1.0992 - val_acc: 0.6710\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 1.0739 - acc: 0.6700 - val_loss: 1.0648 - val_acc: 0.6780\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.0395 - acc: 0.6804 - val_loss: 1.0302 - val_acc: 0.6760\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.0079 - acc: 0.6857 - val_loss: 1.0025 - val_acc: 0.6850\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.9780 - acc: 0.6916 - val_loss: 0.9728 - val_acc: 0.6970\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9500 - acc: 0.6979 - val_loss: 0.9496 - val_acc: 0.6960\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9246 - acc: 0.7008 - val_loss: 0.9242 - val_acc: 0.7020\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9006 - acc: 0.7061 - val_loss: 0.9030 - val_acc: 0.7040\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8786 - acc: 0.7119 - val_loss: 0.8859 - val_acc: 0.7030\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8584 - acc: 0.7183 - val_loss: 0.8655 - val_acc: 0.7100\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8393 - acc: 0.7253 - val_loss: 0.8500 - val_acc: 0.7100\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.8217 - acc: 0.7251 - val_loss: 0.8324 - val_acc: 0.7090\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8058 - acc: 0.7284 - val_loss: 0.8197 - val_acc: 0.7180\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7898 - acc: 0.7341 - val_loss: 0.8065 - val_acc: 0.7210\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.7755 - acc: 0.7388 - val_loss: 0.7952 - val_acc: 0.7160\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.7624 - acc: 0.7396 - val_loss: 0.7843 - val_acc: 0.7250\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.7497 - acc: 0.7428 - val_loss: 0.7726 - val_acc: 0.7210\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.7375 - acc: 0.7452 - val_loss: 0.7640 - val_acc: 0.7220\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7265 - acc: 0.7493 - val_loss: 0.7567 - val_acc: 0.7290\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.7159 - acc: 0.7511 - val_loss: 0.7469 - val_acc: 0.7340\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.7058 - acc: 0.7555 - val_loss: 0.7439 - val_acc: 0.7290\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.6959 - acc: 0.7556 - val_loss: 0.7340 - val_acc: 0.7370\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.6867 - acc: 0.7604 - val_loss: 0.7274 - val_acc: 0.7360\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6777 - acc: 0.7628 - val_loss: 0.7205 - val_acc: 0.7360\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.6697 - acc: 0.7640 - val_loss: 0.7165 - val_acc: 0.7320\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.6615 - acc: 0.7675 - val_loss: 0.7095 - val_acc: 0.7350\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.6538 - acc: 0.7696 - val_loss: 0.7073 - val_acc: 0.7330\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.6465 - acc: 0.7720 - val_loss: 0.6989 - val_acc: 0.7400\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.6387 - acc: 0.7740 - val_loss: 0.6991 - val_acc: 0.7330\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.6318 - acc: 0.7777 - val_loss: 0.6924 - val_acc: 0.7460\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6250 - acc: 0.7792 - val_loss: 0.6886 - val_acc: 0.7410\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.6180 - acc: 0.7819 - val_loss: 0.6893 - val_acc: 0.7490\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.6125 - acc: 0.7841 - val_loss: 0.6825 - val_acc: 0.7470\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.6057 - acc: 0.7868 - val_loss: 0.6789 - val_acc: 0.7510\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.5994 - acc: 0.7868 - val_loss: 0.6753 - val_acc: 0.7470\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.5939 - acc: 0.7909 - val_loss: 0.6722 - val_acc: 0.7450\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.5884 - acc: 0.7945 - val_loss: 0.6700 - val_acc: 0.7470\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.5822 - acc: 0.7947 - val_loss: 0.6670 - val_acc: 0.7500\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.5774 - acc: 0.7991 - val_loss: 0.6633 - val_acc: 0.7540\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.5715 - acc: 0.7999 - val_loss: 0.6635 - val_acc: 0.7430\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.5666 - acc: 0.8029 - val_loss: 0.6595 - val_acc: 0.7590\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.5614 - acc: 0.8044 - val_loss: 0.6573 - val_acc: 0.7470\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.5563 - acc: 0.8077 - val_loss: 0.6573 - val_acc: 0.7520\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.5516 - acc: 0.8100 - val_loss: 0.6548 - val_acc: 0.7480\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.5466 - acc: 0.8112 - val_loss: 0.6536 - val_acc: 0.7560\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.5422 - acc: 0.8120 - val_loss: 0.6502 - val_acc: 0.7570\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.5375 - acc: 0.8145 - val_loss: 0.6478 - val_acc: 0.7570\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5329 - acc: 0.8177 - val_loss: 0.6481 - val_acc: 0.7540\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.5285 - acc: 0.8161 - val_loss: 0.6466 - val_acc: 0.7640\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.5241 - acc: 0.8195 - val_loss: 0.6462 - val_acc: 0.7570\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.5199 - acc: 0.8211 - val_loss: 0.6436 - val_acc: 0.7530\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.5153 - acc: 0.8205 - val_loss: 0.6432 - val_acc: 0.7550\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.5114 - acc: 0.8253 - val_loss: 0.6422 - val_acc: 0.7510\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.5075 - acc: 0.8253 - val_loss: 0.6403 - val_acc: 0.7560\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.5034 - acc: 0.8279 - val_loss: 0.6394 - val_acc: 0.7670\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.4995 - acc: 0.8284 - val_loss: 0.6408 - val_acc: 0.7600\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.4958 - acc: 0.8303 - val_loss: 0.6377 - val_acc: 0.7550\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.4920 - acc: 0.8304 - val_loss: 0.6370 - val_acc: 0.7520\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.4881 - acc: 0.8327 - val_loss: 0.6366 - val_acc: 0.7640\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.4839 - acc: 0.8325 - val_loss: 0.6400 - val_acc: 0.7520\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4805 - acc: 0.8359 - val_loss: 0.6393 - val_acc: 0.7530\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.4764 - acc: 0.8380 - val_loss: 0.6343 - val_acc: 0.7540\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.4728 - acc: 0.8387 - val_loss: 0.6361 - val_acc: 0.7570\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.4694 - acc: 0.8403 - val_loss: 0.6342 - val_acc: 0.7530\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.4658 - acc: 0.8431 - val_loss: 0.6344 - val_acc: 0.7530\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4626 - acc: 0.8455 - val_loss: 0.6334 - val_acc: 0.7580\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.4593 - acc: 0.8444 - val_loss: 0.6332 - val_acc: 0.7500\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.4553 - acc: 0.8452 - val_loss: 0.6325 - val_acc: 0.7540\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4528 - acc: 0.8485 - val_loss: 0.6327 - val_acc: 0.7540\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.4492 - acc: 0.8500 - val_loss: 0.6318 - val_acc: 0.7610\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.4460 - acc: 0.8515 - val_loss: 0.6313 - val_acc: 0.7530\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.4425 - acc: 0.8531 - val_loss: 0.6350 - val_acc: 0.7600\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.4395 - acc: 0.8551 - val_loss: 0.6317 - val_acc: 0.7620\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.4362 - acc: 0.8552 - val_loss: 0.6311 - val_acc: 0.7600\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.4328 - acc: 0.8573 - val_loss: 0.6335 - val_acc: 0.7470\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4298 - acc: 0.8600 - val_loss: 0.6299 - val_acc: 0.7570\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4269 - acc: 0.8591 - val_loss: 0.6291 - val_acc: 0.7600\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4236 - acc: 0.8605 - val_loss: 0.6304 - val_acc: 0.7580\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4209 - acc: 0.8617 - val_loss: 0.6300 - val_acc: 0.7620\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.4177 - acc: 0.8615 - val_loss: 0.6301 - val_acc: 0.7580\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4147 - acc: 0.8621 - val_loss: 0.6297 - val_acc: 0.7570\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4119 - acc: 0.8648 - val_loss: 0.6324 - val_acc: 0.7550\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.4092 - acc: 0.8663 - val_loss: 0.6308 - val_acc: 0.7580\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4058 - acc: 0.8675 - val_loss: 0.6314 - val_acc: 0.7550\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.4036 - acc: 0.8683 - val_loss: 0.6309 - val_acc: 0.7540\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.4003 - acc: 0.8709 - val_loss: 0.6307 - val_acc: 0.7580\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.3976 - acc: 0.8696 - val_loss: 0.6299 - val_acc: 0.7550\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.3949 - acc: 0.8711 - val_loss: 0.6336 - val_acc: 0.7540\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.3924 - acc: 0.8731 - val_loss: 0.6305 - val_acc: 0.7600\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.3895 - acc: 0.8743 - val_loss: 0.6313 - val_acc: 0.7580\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.3868 - acc: 0.8775 - val_loss: 0.6312 - val_acc: 0.7610\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.3844 - acc: 0.8748 - val_loss: 0.6301 - val_acc: 0.7600\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.3811 - acc: 0.8781 - val_loss: 0.6331 - val_acc: 0.7540\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.3789 - acc: 0.8791 - val_loss: 0.6310 - val_acc: 0.7550\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.3762 - acc: 0.8805 - val_loss: 0.6303 - val_acc: 0.7560\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.3741 - acc: 0.8808 - val_loss: 0.6322 - val_acc: 0.7560\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.3714 - acc: 0.8812 - val_loss: 0.6334 - val_acc: 0.7570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 0.3689 - acc: 0.8832 - val_loss: 0.6336 - val_acc: 0.7550\n"
     ]
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 20us/step\n"
     ]
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 23us/step\n"
     ]
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.36572940867741904, 0.8845333333015442]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6589062994321188, 0.7606666666666667]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interpret these results, run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __SOLUTION__\n",
    "model.metrics_names"

   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first element of the list returned by `model.evaluate` is the loss, and the second is the accuracy score. \n",
    "\n",
    "Note that the result you obtained here isn't exactly the same as before. This because the training set is slightly different! You removed 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss function versus the number of epochs. Be sure to include the training and the validation loss in the same plot. Then, create a second plot comparing training and validation accuracy to the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a limit around the 60th epoch. This means that you're probably **overfitting** the model to the training data when you train for many epochs past this dropoff point of around 40 epochs. Luckily, you learned how to tackle overfitting in the previous lecture! Since it seems clear that you are training too long, include early stopping at the 60th epoch first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "Below, observe how to update the model to include an earlier cutoff point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9477 - acc: 0.1487 - val_loss: 1.9288 - val_acc: 0.1730\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.9137 - acc: 0.1896 - val_loss: 1.9019 - val_acc: 0.2030\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8861 - acc: 0.2344 - val_loss: 1.8738 - val_acc: 0.2360\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.8564 - acc: 0.2651 - val_loss: 1.8427 - val_acc: 0.2690\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.8229 - acc: 0.2975 - val_loss: 1.8066 - val_acc: 0.3010\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.7838 - acc: 0.3292 - val_loss: 1.7642 - val_acc: 0.3310\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.7382 - acc: 0.3668 - val_loss: 1.7151 - val_acc: 0.3730\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6871 - acc: 0.4040 - val_loss: 1.6609 - val_acc: 0.4060\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6319 - acc: 0.4411 - val_loss: 1.6053 - val_acc: 0.4250\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5736 - acc: 0.4669 - val_loss: 1.5483 - val_acc: 0.4550\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5148 - acc: 0.4999 - val_loss: 1.4912 - val_acc: 0.4890\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4563 - acc: 0.5285 - val_loss: 1.4372 - val_acc: 0.5040\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3985 - acc: 0.5540 - val_loss: 1.3799 - val_acc: 0.5370\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3418 - acc: 0.5745 - val_loss: 1.3280 - val_acc: 0.5550\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2879 - acc: 0.5955 - val_loss: 1.2769 - val_acc: 0.5880\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2362 - acc: 0.6115 - val_loss: 1.2305 - val_acc: 0.5990\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1878 - acc: 0.6275 - val_loss: 1.1868 - val_acc: 0.6190\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.1427 - acc: 0.6423 - val_loss: 1.1440 - val_acc: 0.6370\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.1005 - acc: 0.6547 - val_loss: 1.1053 - val_acc: 0.6490\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.0616 - acc: 0.6681 - val_loss: 1.0697 - val_acc: 0.6660\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0256 - acc: 0.6763 - val_loss: 1.0378 - val_acc: 0.6680\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9917 - acc: 0.6852 - val_loss: 1.0092 - val_acc: 0.6670\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9610 - acc: 0.6929 - val_loss: 0.9807 - val_acc: 0.6700\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9326 - acc: 0.6987 - val_loss: 0.9550 - val_acc: 0.6860\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9063 - acc: 0.7063 - val_loss: 0.9300 - val_acc: 0.6900\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8818 - acc: 0.7128 - val_loss: 0.9089 - val_acc: 0.6960\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8596 - acc: 0.7163 - val_loss: 0.8916 - val_acc: 0.7180\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8389 - acc: 0.7217 - val_loss: 0.8714 - val_acc: 0.7210\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8198 - acc: 0.7272 - val_loss: 0.8551 - val_acc: 0.7090\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8018 - acc: 0.7305 - val_loss: 0.8391 - val_acc: 0.7140\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7849 - acc: 0.7347 - val_loss: 0.8255 - val_acc: 0.7150\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.7693 - acc: 0.7383 - val_loss: 0.8122 - val_acc: 0.7230\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.7547 - acc: 0.7441 - val_loss: 0.8042 - val_acc: 0.7300\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.7411 - acc: 0.7457 - val_loss: 0.7905 - val_acc: 0.7260\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.7281 - acc: 0.7516 - val_loss: 0.7811 - val_acc: 0.7380\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7156 - acc: 0.7549 - val_loss: 0.7718 - val_acc: 0.7330\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.7042 - acc: 0.7557 - val_loss: 0.7637 - val_acc: 0.7350\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6929 - acc: 0.7611 - val_loss: 0.7548 - val_acc: 0.7410\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6825 - acc: 0.7636 - val_loss: 0.7510 - val_acc: 0.7380\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6728 - acc: 0.7679 - val_loss: 0.7407 - val_acc: 0.7500\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.6634 - acc: 0.7728 - val_loss: 0.7348 - val_acc: 0.7500\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.6549 - acc: 0.7723 - val_loss: 0.7281 - val_acc: 0.7500\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.6459 - acc: 0.7769 - val_loss: 0.7219 - val_acc: 0.7510\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6375 - acc: 0.7796 - val_loss: 0.7186 - val_acc: 0.7510\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.6295 - acc: 0.7784 - val_loss: 0.7114 - val_acc: 0.7520\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.6218 - acc: 0.7841 - val_loss: 0.7102 - val_acc: 0.7570\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.6151 - acc: 0.7847 - val_loss: 0.7059 - val_acc: 0.7480\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.6078 - acc: 0.7883 - val_loss: 0.6986 - val_acc: 0.7550\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.6011 - acc: 0.7909 - val_loss: 0.6945 - val_acc: 0.7570\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.5940 - acc: 0.7921 - val_loss: 0.6902 - val_acc: 0.7560\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.5881 - acc: 0.7952 - val_loss: 0.6890 - val_acc: 0.7540\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.5815 - acc: 0.7957 - val_loss: 0.6852 - val_acc: 0.7550\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5756 - acc: 0.8000 - val_loss: 0.6827 - val_acc: 0.7580\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.5697 - acc: 0.8016 - val_loss: 0.6794 - val_acc: 0.7570\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5643 - acc: 0.8045 - val_loss: 0.6773 - val_acc: 0.7570\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.5586 - acc: 0.8064 - val_loss: 0.6736 - val_acc: 0.7650\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.5530 - acc: 0.8097 - val_loss: 0.6716 - val_acc: 0.7610\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5477 - acc: 0.8120 - val_loss: 0.6688 - val_acc: 0.7580\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.5427 - acc: 0.8117 - val_loss: 0.6664 - val_acc: 0.7640\n",
      "Epoch 60/60\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.5373 - acc: 0.8131 - val_loss: 0.6646 - val_acc: 0.7610\n"
     ]
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step\n"
     ]
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 20us/step\n"
     ]
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5326700656572978, 0.8154666666666667]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6795797921816508, 0.7439999996821086]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! your test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs model you originally fit.\n",
    "\n",
    "Now, take a look at how regularization techniques can further improve your model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, take a look at L2 regularization. Keras makes L2 regularization easy. Simply add the `kernel_regularizer=keras.regularizers.l2(lambda_coeff)` parameter to any model layer. The `lambda_coeff` parameter determines the strength of the regularization you wish to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.5921 - acc: 0.1567 - val_loss: 2.5850 - val_acc: 0.1640\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.5698 - acc: 0.1959 - val_loss: 2.5664 - val_acc: 0.1860\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.5494 - acc: 0.2169 - val_loss: 2.5476 - val_acc: 0.2060\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.5274 - acc: 0.2321 - val_loss: 2.5265 - val_acc: 0.2270\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.5025 - acc: 0.2508 - val_loss: 2.5023 - val_acc: 0.2480\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.4744 - acc: 0.2737 - val_loss: 2.4744 - val_acc: 0.2640\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.4428 - acc: 0.2993 - val_loss: 2.4420 - val_acc: 0.2910\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.4078 - acc: 0.3293 - val_loss: 2.4061 - val_acc: 0.3070\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.3691 - acc: 0.3585 - val_loss: 2.3662 - val_acc: 0.3300\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.3269 - acc: 0.3832 - val_loss: 2.3229 - val_acc: 0.3610\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.2823 - acc: 0.4083 - val_loss: 2.2776 - val_acc: 0.3860\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.2358 - acc: 0.4289 - val_loss: 2.2319 - val_acc: 0.4130\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.1894 - acc: 0.4473 - val_loss: 2.1851 - val_acc: 0.4380\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.1425 - acc: 0.4705 - val_loss: 2.1399 - val_acc: 0.4530\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.0965 - acc: 0.4948 - val_loss: 2.0944 - val_acc: 0.4840\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.0513 - acc: 0.5165 - val_loss: 2.0505 - val_acc: 0.4970\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.0068 - acc: 0.5387 - val_loss: 2.0073 - val_acc: 0.5120\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.9637 - acc: 0.5597 - val_loss: 1.9657 - val_acc: 0.5370\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.9221 - acc: 0.5777 - val_loss: 1.9262 - val_acc: 0.5540\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8817 - acc: 0.5941 - val_loss: 1.8866 - val_acc: 0.5850\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.8430 - acc: 0.6116 - val_loss: 1.8501 - val_acc: 0.5970\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.8053 - acc: 0.6243 - val_loss: 1.8134 - val_acc: 0.5990\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7693 - acc: 0.6371 - val_loss: 1.7793 - val_acc: 0.6250\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7344 - acc: 0.6539 - val_loss: 1.7453 - val_acc: 0.6310\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7014 - acc: 0.6613 - val_loss: 1.7131 - val_acc: 0.6450\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6695 - acc: 0.6723 - val_loss: 1.6831 - val_acc: 0.6480\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6394 - acc: 0.6787 - val_loss: 1.6539 - val_acc: 0.6590\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6101 - acc: 0.6903 - val_loss: 1.6272 - val_acc: 0.6600\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5826 - acc: 0.6960 - val_loss: 1.6014 - val_acc: 0.6670\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.5569 - acc: 0.7019 - val_loss: 1.5764 - val_acc: 0.6710\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5321 - acc: 0.7067 - val_loss: 1.5536 - val_acc: 0.6770\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5088 - acc: 0.7143 - val_loss: 1.5317 - val_acc: 0.6760\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.4862 - acc: 0.7193 - val_loss: 1.5165 - val_acc: 0.6820\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.4657 - acc: 0.7224 - val_loss: 1.4921 - val_acc: 0.6860\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4454 - acc: 0.7251 - val_loss: 1.4767 - val_acc: 0.6880\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4263 - acc: 0.7303 - val_loss: 1.4572 - val_acc: 0.6850\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4088 - acc: 0.7332 - val_loss: 1.4415 - val_acc: 0.6940\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3916 - acc: 0.7379 - val_loss: 1.4261 - val_acc: 0.6990\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3758 - acc: 0.7403 - val_loss: 1.4127 - val_acc: 0.7050\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 1.3605 - acc: 0.7451 - val_loss: 1.3989 - val_acc: 0.7050\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3461 - acc: 0.7489 - val_loss: 1.3854 - val_acc: 0.7040\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3322 - acc: 0.7516 - val_loss: 1.3743 - val_acc: 0.7120\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3192 - acc: 0.7556 - val_loss: 1.3621 - val_acc: 0.7090\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.3067 - acc: 0.7563 - val_loss: 1.3517 - val_acc: 0.7110\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.2944 - acc: 0.7587 - val_loss: 1.3443 - val_acc: 0.7200\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.2832 - acc: 0.7615 - val_loss: 1.3308 - val_acc: 0.7220\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2715 - acc: 0.7657 - val_loss: 1.3224 - val_acc: 0.7270\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2611 - acc: 0.7664 - val_loss: 1.3139 - val_acc: 0.7270\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2507 - acc: 0.7707 - val_loss: 1.3056 - val_acc: 0.7300\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.2405 - acc: 0.7720 - val_loss: 1.2965 - val_acc: 0.7330\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 1.2312 - acc: 0.7773 - val_loss: 1.2895 - val_acc: 0.7330\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2217 - acc: 0.7768 - val_loss: 1.2821 - val_acc: 0.7300\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2132 - acc: 0.7785 - val_loss: 1.2746 - val_acc: 0.7310\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 1.2043 - acc: 0.7797 - val_loss: 1.2684 - val_acc: 0.7420\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 1.1956 - acc: 0.7817 - val_loss: 1.2611 - val_acc: 0.7390\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 1.1874 - acc: 0.7831 - val_loss: 1.2570 - val_acc: 0.7420\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.1799 - acc: 0.7864 - val_loss: 1.2488 - val_acc: 0.7390\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.1715 - acc: 0.7901 - val_loss: 1.2442 - val_acc: 0.7400\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1640 - acc: 0.7888 - val_loss: 1.2379 - val_acc: 0.7380\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1564 - acc: 0.7945 - val_loss: 1.2324 - val_acc: 0.7480\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1495 - acc: 0.7951 - val_loss: 1.2262 - val_acc: 0.7440\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1423 - acc: 0.7980 - val_loss: 1.2207 - val_acc: 0.7520\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.1354 - acc: 0.7992 - val_loss: 1.2156 - val_acc: 0.7550\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.1284 - acc: 0.8000 - val_loss: 1.2109 - val_acc: 0.7530\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1222 - acc: 0.8048 - val_loss: 1.2067 - val_acc: 0.7490\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1155 - acc: 0.8059 - val_loss: 1.2018 - val_acc: 0.7490\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1086 - acc: 0.8048 - val_loss: 1.1960 - val_acc: 0.7540\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1024 - acc: 0.8084 - val_loss: 1.1917 - val_acc: 0.7570\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0962 - acc: 0.8092 - val_loss: 1.1876 - val_acc: 0.7600\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.0903 - acc: 0.8099 - val_loss: 1.1832 - val_acc: 0.7590\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0843 - acc: 0.8116 - val_loss: 1.1802 - val_acc: 0.7510\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.0786 - acc: 0.8147 - val_loss: 1.1753 - val_acc: 0.7600\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 1.0728 - acc: 0.8160 - val_loss: 1.1714 - val_acc: 0.7580\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 12us/step - loss: 1.0669 - acc: 0.8191 - val_loss: 1.1673 - val_acc: 0.7600\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0615 - acc: 0.8205 - val_loss: 1.1637 - val_acc: 0.7580\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0557 - acc: 0.8200 - val_loss: 1.1594 - val_acc: 0.7620\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.0501 - acc: 0.8220 - val_loss: 1.1574 - val_acc: 0.7580\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.0449 - acc: 0.8260 - val_loss: 1.1525 - val_acc: 0.7650\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0398 - acc: 0.8255 - val_loss: 1.1509 - val_acc: 0.7590\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0343 - acc: 0.8261 - val_loss: 1.1460 - val_acc: 0.7620\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0293 - acc: 0.8253 - val_loss: 1.1417 - val_acc: 0.7690\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.0241 - acc: 0.8292 - val_loss: 1.1390 - val_acc: 0.7650\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 1.0190 - acc: 0.8311 - val_loss: 1.1356 - val_acc: 0.7640\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0144 - acc: 0.8303 - val_loss: 1.1334 - val_acc: 0.7620\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0092 - acc: 0.8321 - val_loss: 1.1299 - val_acc: 0.7690\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0045 - acc: 0.8329 - val_loss: 1.1262 - val_acc: 0.7660\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9997 - acc: 0.8355 - val_loss: 1.1237 - val_acc: 0.7660\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9950 - acc: 0.8363 - val_loss: 1.1208 - val_acc: 0.7730\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9906 - acc: 0.8373 - val_loss: 1.1182 - val_acc: 0.7690\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9862 - acc: 0.8375 - val_loss: 1.1134 - val_acc: 0.7750\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9811 - acc: 0.8396 - val_loss: 1.1114 - val_acc: 0.7720\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9768 - acc: 0.8395 - val_loss: 1.1088 - val_acc: 0.7690\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9722 - acc: 0.8420 - val_loss: 1.1057 - val_acc: 0.7770\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9678 - acc: 0.8416 - val_loss: 1.1051 - val_acc: 0.7740\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9636 - acc: 0.8431 - val_loss: 1.1009 - val_acc: 0.7710\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9591 - acc: 0.8447 - val_loss: 1.0981 - val_acc: 0.7700\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9551 - acc: 0.8452 - val_loss: 1.0963 - val_acc: 0.7770\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9508 - acc: 0.8488 - val_loss: 1.0923 - val_acc: 0.7720\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9467 - acc: 0.8480 - val_loss: 1.0897 - val_acc: 0.7750\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9422 - acc: 0.8501 - val_loss: 1.0871 - val_acc: 0.7760\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9379 - acc: 0.8507 - val_loss: 1.0872 - val_acc: 0.7760\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9342 - acc: 0.8500 - val_loss: 1.0831 - val_acc: 0.7730\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9300 - acc: 0.8536 - val_loss: 1.0797 - val_acc: 0.7790\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9260 - acc: 0.8548 - val_loss: 1.0781 - val_acc: 0.7780\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9222 - acc: 0.8539 - val_loss: 1.0767 - val_acc: 0.7740\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9179 - acc: 0.8545 - val_loss: 1.0732 - val_acc: 0.7800\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9142 - acc: 0.8577 - val_loss: 1.0698 - val_acc: 0.7820\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9104 - acc: 0.8580 - val_loss: 1.0676 - val_acc: 0.7790\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9066 - acc: 0.8607 - val_loss: 1.0660 - val_acc: 0.7770\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.9029 - acc: 0.8601 - val_loss: 1.0667 - val_acc: 0.7800\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8993 - acc: 0.8624 - val_loss: 1.0622 - val_acc: 0.7770\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8950 - acc: 0.8620 - val_loss: 1.0609 - val_acc: 0.7820\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8915 - acc: 0.8652 - val_loss: 1.0580 - val_acc: 0.7790\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8878 - acc: 0.8652 - val_loss: 1.0567 - val_acc: 0.7760\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8844 - acc: 0.8677 - val_loss: 1.0534 - val_acc: 0.7840\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8806 - acc: 0.8643 - val_loss: 1.0511 - val_acc: 0.7870\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8772 - acc: 0.8692 - val_loss: 1.0499 - val_acc: 0.7790\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8740 - acc: 0.8697 - val_loss: 1.0493 - val_acc: 0.7790\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8700 - acc: 0.8712 - val_loss: 1.0465 - val_acc: 0.7870\n"

     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8666 - acc: 0.8723 - val_loss: 1.0436 - val_acc: 0.7890\n"
     ]
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. Notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 15.9796 - acc: 0.1627 - val_loss: 15.5710 - val_acc: 0.1460\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 15.2173 - acc: 0.1788 - val_loss: 14.8255 - val_acc: 0.1790\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 14.4813 - acc: 0.2037 - val_loss: 14.1016 - val_acc: 0.2020\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 13.7662 - acc: 0.2288 - val_loss: 13.3963 - val_acc: 0.2230\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 13.0705 - acc: 0.2579 - val_loss: 12.7102 - val_acc: 0.2730\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 12.3944 - acc: 0.2924 - val_loss: 12.0434 - val_acc: 0.3070\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 11.7371 - acc: 0.3287 - val_loss: 11.3953 - val_acc: 0.3420\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 11.0991 - acc: 0.3684 - val_loss: 10.7670 - val_acc: 0.3910\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 10.4811 - acc: 0.4105 - val_loss: 10.1588 - val_acc: 0.4520\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 9.8834 - acc: 0.4467 - val_loss: 9.5716 - val_acc: 0.4770\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 9.3070 - acc: 0.4689 - val_loss: 9.0056 - val_acc: 0.5190\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 8.7520 - acc: 0.4991 - val_loss: 8.4611 - val_acc: 0.5370\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 8.2183 - acc: 0.5209 - val_loss: 7.9383 - val_acc: 0.5560\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 7.7066 - acc: 0.5416 - val_loss: 7.4384 - val_acc: 0.5620\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 7.2177 - acc: 0.5536 - val_loss: 6.9622 - val_acc: 0.5660\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 6.7523 - acc: 0.5657 - val_loss: 6.5078 - val_acc: 0.5730\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 6.3090 - acc: 0.5799 - val_loss: 6.0762 - val_acc: 0.5770\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 5.8880 - acc: 0.5887 - val_loss: 5.6693 - val_acc: 0.5940\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 5.4904 - acc: 0.6005 - val_loss: 5.2825 - val_acc: 0.5960\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 5.1151 - acc: 0.6148 - val_loss: 4.9184 - val_acc: 0.6120\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 4.7622 - acc: 0.6176 - val_loss: 4.5774 - val_acc: 0.6200\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 4.4311 - acc: 0.6248 - val_loss: 4.2584 - val_acc: 0.6300\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 4.1228 - acc: 0.6341 - val_loss: 3.9611 - val_acc: 0.6410\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 3.8381 - acc: 0.6408 - val_loss: 3.6884 - val_acc: 0.6330\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 3.5757 - acc: 0.6425 - val_loss: 3.4383 - val_acc: 0.6430\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 3.3357 - acc: 0.6475 - val_loss: 3.2104 - val_acc: 0.6490\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 3.1180 - acc: 0.6479 - val_loss: 3.0026 - val_acc: 0.6590\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.9218 - acc: 0.6563 - val_loss: 2.8177 - val_acc: 0.6630\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.7474 - acc: 0.6557 - val_loss: 2.6530 - val_acc: 0.6740\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.5935 - acc: 0.6636 - val_loss: 2.5095 - val_acc: 0.6690\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.4601 - acc: 0.6637 - val_loss: 2.3879 - val_acc: 0.6700\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.3467 - acc: 0.6652 - val_loss: 2.2822 - val_acc: 0.6720\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.2526 - acc: 0.6667 - val_loss: 2.1983 - val_acc: 0.6660\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.1769 - acc: 0.6665 - val_loss: 2.1302 - val_acc: 0.6690\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.1182 - acc: 0.6691 - val_loss: 2.0786 - val_acc: 0.6680\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.0744 - acc: 0.6689 - val_loss: 2.0416 - val_acc: 0.6700\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.0423 - acc: 0.6695 - val_loss: 2.0136 - val_acc: 0.6700\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.0163 - acc: 0.6708 - val_loss: 1.9868 - val_acc: 0.6810\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.9932 - acc: 0.6729 - val_loss: 1.9660 - val_acc: 0.6770\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.9723 - acc: 0.6716 - val_loss: 1.9448 - val_acc: 0.6760\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.9523 - acc: 0.6743 - val_loss: 1.9250 - val_acc: 0.6800\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.9340 - acc: 0.6725 - val_loss: 1.9054 - val_acc: 0.6850\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.9168 - acc: 0.6760 - val_loss: 1.8926 - val_acc: 0.6850\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.9007 - acc: 0.6763 - val_loss: 1.8718 - val_acc: 0.6820\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8842 - acc: 0.6779 - val_loss: 1.8568 - val_acc: 0.6880\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8687 - acc: 0.6792 - val_loss: 1.8380 - val_acc: 0.6900\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8541 - acc: 0.6820 - val_loss: 1.8240 - val_acc: 0.6930\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8395 - acc: 0.6808 - val_loss: 1.8081 - val_acc: 0.6940\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8263 - acc: 0.6837 - val_loss: 1.7973 - val_acc: 0.6930\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.8124 - acc: 0.6852 - val_loss: 1.7852 - val_acc: 0.6930\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.7997 - acc: 0.6860 - val_loss: 1.7679 - val_acc: 0.6960\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7872 - acc: 0.6863 - val_loss: 1.7584 - val_acc: 0.7020\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7748 - acc: 0.6876 - val_loss: 1.7445 - val_acc: 0.7010\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7633 - acc: 0.6892 - val_loss: 1.7318 - val_acc: 0.6980\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.7519 - acc: 0.6905 - val_loss: 1.7211 - val_acc: 0.7000\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.7405 - acc: 0.6901 - val_loss: 1.7119 - val_acc: 0.6920\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7300 - acc: 0.6935 - val_loss: 1.6975 - val_acc: 0.7000\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7188 - acc: 0.6941 - val_loss: 1.6861 - val_acc: 0.7000\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7082 - acc: 0.6948 - val_loss: 1.6760 - val_acc: 0.6990\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6980 - acc: 0.6952 - val_loss: 1.6681 - val_acc: 0.7000\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6876 - acc: 0.6956 - val_loss: 1.6627 - val_acc: 0.6980\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6784 - acc: 0.6976 - val_loss: 1.6480 - val_acc: 0.7080\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6691 - acc: 0.6964 - val_loss: 1.6410 - val_acc: 0.7030\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6591 - acc: 0.6989 - val_loss: 1.6289 - val_acc: 0.7040\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6500 - acc: 0.6987 - val_loss: 1.6179 - val_acc: 0.7060\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6411 - acc: 0.6993 - val_loss: 1.6120 - val_acc: 0.7050\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6325 - acc: 0.7028 - val_loss: 1.6042 - val_acc: 0.7170\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6234 - acc: 0.7023 - val_loss: 1.5936 - val_acc: 0.7080\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6148 - acc: 0.7020 - val_loss: 1.5839 - val_acc: 0.7100\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6065 - acc: 0.7033 - val_loss: 1.5748 - val_acc: 0.7090\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5987 - acc: 0.7041 - val_loss: 1.5686 - val_acc: 0.7110\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5907 - acc: 0.7036 - val_loss: 1.5585 - val_acc: 0.7120\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.5819 - acc: 0.7048 - val_loss: 1.5523 - val_acc: 0.7130\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5745 - acc: 0.7059 - val_loss: 1.5458 - val_acc: 0.7130\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5669 - acc: 0.7059 - val_loss: 1.5359 - val_acc: 0.7070\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.5592 - acc: 0.7052 - val_loss: 1.5363 - val_acc: 0.7090\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5518 - acc: 0.7072 - val_loss: 1.5207 - val_acc: 0.7170\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5437 - acc: 0.7084 - val_loss: 1.5138 - val_acc: 0.7160\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5364 - acc: 0.7081 - val_loss: 1.5059 - val_acc: 0.7100\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5292 - acc: 0.7068 - val_loss: 1.4993 - val_acc: 0.7150\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5219 - acc: 0.7085 - val_loss: 1.4942 - val_acc: 0.7140\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5149 - acc: 0.7099 - val_loss: 1.4855 - val_acc: 0.7160\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5077 - acc: 0.7101 - val_loss: 1.4859 - val_acc: 0.7100\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.5013 - acc: 0.7104 - val_loss: 1.4709 - val_acc: 0.7140\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4942 - acc: 0.7123 - val_loss: 1.4629 - val_acc: 0.7170\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4873 - acc: 0.7116 - val_loss: 1.4567 - val_acc: 0.7160\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4807 - acc: 0.7128 - val_loss: 1.4498 - val_acc: 0.7180\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4741 - acc: 0.7113 - val_loss: 1.4458 - val_acc: 0.7200\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4674 - acc: 0.7145 - val_loss: 1.4378 - val_acc: 0.7220\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4610 - acc: 0.7141 - val_loss: 1.4309 - val_acc: 0.7160\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4543 - acc: 0.7139 - val_loss: 1.4271 - val_acc: 0.7180\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4481 - acc: 0.7151 - val_loss: 1.4241 - val_acc: 0.7150\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4426 - acc: 0.7149 - val_loss: 1.4142 - val_acc: 0.7200\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.4363 - acc: 0.7157 - val_loss: 1.4057 - val_acc: 0.7220\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4297 - acc: 0.7175 - val_loss: 1.4016 - val_acc: 0.7220\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4235 - acc: 0.7157 - val_loss: 1.3969 - val_acc: 0.7170\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4183 - acc: 0.7176 - val_loss: 1.3934 - val_acc: 0.7170\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4127 - acc: 0.7171 - val_loss: 1.3846 - val_acc: 0.7210\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4071 - acc: 0.7184 - val_loss: 1.3792 - val_acc: 0.7230\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4010 - acc: 0.7185 - val_loss: 1.3727 - val_acc: 0.7250\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3953 - acc: 0.7199 - val_loss: 1.3676 - val_acc: 0.7230\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3899 - acc: 0.7199 - val_loss: 1.3630 - val_acc: 0.7250\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3839 - acc: 0.7191 - val_loss: 1.3587 - val_acc: 0.7270\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3786 - acc: 0.7184 - val_loss: 1.3498 - val_acc: 0.7260\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3733 - acc: 0.7207 - val_loss: 1.3476 - val_acc: 0.7230\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3682 - acc: 0.7211 - val_loss: 1.3406 - val_acc: 0.7310\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3630 - acc: 0.7220 - val_loss: 1.3410 - val_acc: 0.7220\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3579 - acc: 0.7213 - val_loss: 1.3310 - val_acc: 0.7290\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3522 - acc: 0.7229 - val_loss: 1.3241 - val_acc: 0.7270\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3476 - acc: 0.7211 - val_loss: 1.3213 - val_acc: 0.7310\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3434 - acc: 0.7211 - val_loss: 1.3167 - val_acc: 0.7290\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3370 - acc: 0.7232 - val_loss: 1.3130 - val_acc: 0.7290\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3324 - acc: 0.7257 - val_loss: 1.3118 - val_acc: 0.7250\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3285 - acc: 0.7253 - val_loss: 1.3017 - val_acc: 0.7300\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3228 - acc: 0.7252 - val_loss: 1.2974 - val_acc: 0.7270\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3179 - acc: 0.7237 - val_loss: 1.2956 - val_acc: 0.7230\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3138 - acc: 0.7245 - val_loss: 1.2914 - val_acc: 0.7280\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.3091 - acc: 0.7272 - val_loss: 1.2828 - val_acc: 0.7370\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3041 - acc: 0.7252 - val_loss: 1.2797 - val_acc: 0.7320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2998 - acc: 0.7260 - val_loss: 1.2743 - val_acc: 0.7330\n"
     ]
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the training and validation accuracy don't diverge as much as before. Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like you can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 16.0274 - acc: 0.1509 - val_loss: 15.6090 - val_acc: 0.1830\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 15.2518 - acc: 0.2101 - val_loss: 14.8593 - val_acc: 0.2300\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 14.5123 - acc: 0.2427 - val_loss: 14.1349 - val_acc: 0.2490\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 13.7952 - acc: 0.2671 - val_loss: 13.4313 - val_acc: 0.2580\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 13.0985 - acc: 0.2865 - val_loss: 12.7468 - val_acc: 0.2730\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 12.4211 - acc: 0.3085 - val_loss: 12.0808 - val_acc: 0.2840\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 11.7624 - acc: 0.3340 - val_loss: 11.4348 - val_acc: 0.2970\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 11.1228 - acc: 0.3593 - val_loss: 10.8059 - val_acc: 0.3240\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 10.5030 - acc: 0.3837 - val_loss: 10.1971 - val_acc: 0.3580\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 9.9031 - acc: 0.4139 - val_loss: 9.6082 - val_acc: 0.3940\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 9.3244 - acc: 0.4337 - val_loss: 9.0404 - val_acc: 0.4400\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 8.7676 - acc: 0.4657 - val_loss: 8.4948 - val_acc: 0.4680\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 8.2335 - acc: 0.4843 - val_loss: 7.9730 - val_acc: 0.4900\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 7.7227 - acc: 0.5056 - val_loss: 7.4752 - val_acc: 0.4910\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 7.2350 - acc: 0.5224 - val_loss: 6.9996 - val_acc: 0.5020\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 6.7700 - acc: 0.5429 - val_loss: 6.5474 - val_acc: 0.5060\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 6.3278 - acc: 0.5532 - val_loss: 6.1164 - val_acc: 0.5560\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 5.9088 - acc: 0.5732 - val_loss: 5.7112 - val_acc: 0.5460\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 5.5130 - acc: 0.5853 - val_loss: 5.3278 - val_acc: 0.5680\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 5.1403 - acc: 0.5957 - val_loss: 4.9652 - val_acc: 0.5870\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 4.7907 - acc: 0.6111 - val_loss: 4.6281 - val_acc: 0.5900\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 4.4631 - acc: 0.6163 - val_loss: 4.3137 - val_acc: 0.5730\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 4.1577 - acc: 0.6201 - val_loss: 4.0188 - val_acc: 0.5940\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 3.8753 - acc: 0.6313 - val_loss: 3.7495 - val_acc: 0.5960\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 3.6158 - acc: 0.6339 - val_loss: 3.4981 - val_acc: 0.6240\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 3.3785 - acc: 0.6432 - val_loss: 3.2716 - val_acc: 0.6390\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 3.1627 - acc: 0.6496 - val_loss: 3.0665 - val_acc: 0.6400\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.9672 - acc: 0.6548 - val_loss: 2.8825 - val_acc: 0.6340\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.7928 - acc: 0.6549 - val_loss: 2.7185 - val_acc: 0.6480\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.6407 - acc: 0.6569 - val_loss: 2.5759 - val_acc: 0.6400\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.5090 - acc: 0.6616 - val_loss: 2.4563 - val_acc: 0.6420\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.3976 - acc: 0.6577 - val_loss: 2.3529 - val_acc: 0.6500\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.3048 - acc: 0.6617 - val_loss: 2.2690 - val_acc: 0.6450\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.2299 - acc: 0.6616 - val_loss: 2.2018 - val_acc: 0.6640\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.1711 - acc: 0.6623 - val_loss: 2.1522 - val_acc: 0.6480\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.1270 - acc: 0.6639 - val_loss: 2.1113 - val_acc: 0.6610\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.0934 - acc: 0.6657 - val_loss: 2.0827 - val_acc: 0.6550\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 2.0668 - acc: 0.6652 - val_loss: 2.0577 - val_acc: 0.6630\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.0430 - acc: 0.6667 - val_loss: 2.0327 - val_acc: 0.6780\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 2.0210 - acc: 0.6716 - val_loss: 2.0104 - val_acc: 0.6790\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0009 - acc: 0.6721 - val_loss: 1.9945 - val_acc: 0.6690\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9827 - acc: 0.6737 - val_loss: 1.9740 - val_acc: 0.6780\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9646 - acc: 0.6764 - val_loss: 1.9564 - val_acc: 0.6740\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9483 - acc: 0.6763 - val_loss: 1.9425 - val_acc: 0.6740\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.9323 - acc: 0.6760 - val_loss: 1.9231 - val_acc: 0.6810\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.9165 - acc: 0.6761 - val_loss: 1.9063 - val_acc: 0.6880\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.9018 - acc: 0.6781 - val_loss: 1.8916 - val_acc: 0.6890\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.8874 - acc: 0.6787 - val_loss: 1.8810 - val_acc: 0.6800\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.8742 - acc: 0.6789 - val_loss: 1.8635 - val_acc: 0.6910\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.8612 - acc: 0.6833 - val_loss: 1.8499 - val_acc: 0.6830\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8478 - acc: 0.6837 - val_loss: 1.8381 - val_acc: 0.6880\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8357 - acc: 0.6820 - val_loss: 1.8259 - val_acc: 0.6890\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.8236 - acc: 0.6825 - val_loss: 1.8141 - val_acc: 0.6870\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.8113 - acc: 0.6867 - val_loss: 1.8037 - val_acc: 0.6860\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.8005 - acc: 0.6852 - val_loss: 1.7891 - val_acc: 0.6870\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7887 - acc: 0.6875 - val_loss: 1.7787 - val_acc: 0.6920\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7780 - acc: 0.6849 - val_loss: 1.7696 - val_acc: 0.6870\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7675 - acc: 0.6893 - val_loss: 1.7603 - val_acc: 0.6880\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7569 - acc: 0.6909 - val_loss: 1.7458 - val_acc: 0.6850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7468 - acc: 0.6927 - val_loss: 1.7368 - val_acc: 0.6850\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7360 - acc: 0.6923 - val_loss: 1.7292 - val_acc: 0.6890\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.7264 - acc: 0.6931 - val_loss: 1.7187 - val_acc: 0.6900\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7168 - acc: 0.6924 - val_loss: 1.7071 - val_acc: 0.6930\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.7075 - acc: 0.6932 - val_loss: 1.7007 - val_acc: 0.6910\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6982 - acc: 0.6949 - val_loss: 1.6887 - val_acc: 0.6940\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6890 - acc: 0.6944 - val_loss: 1.6850 - val_acc: 0.6950\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6805 - acc: 0.6969 - val_loss: 1.6675 - val_acc: 0.7010\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6709 - acc: 0.6961 - val_loss: 1.6610 - val_acc: 0.6980\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6625 - acc: 0.6960 - val_loss: 1.6548 - val_acc: 0.6980\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6538 - acc: 0.7004 - val_loss: 1.6428 - val_acc: 0.7100\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.6455 - acc: 0.6969 - val_loss: 1.6360 - val_acc: 0.7050\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6366 - acc: 0.6983 - val_loss: 1.6260 - val_acc: 0.7040\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6284 - acc: 0.6993 - val_loss: 1.6161 - val_acc: 0.7040\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6203 - acc: 0.7012 - val_loss: 1.6088 - val_acc: 0.7070\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.6121 - acc: 0.7016 - val_loss: 1.6031 - val_acc: 0.7050\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.6049 - acc: 0.7003 - val_loss: 1.5935 - val_acc: 0.7100\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5963 - acc: 0.7032 - val_loss: 1.5849 - val_acc: 0.7070\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5885 - acc: 0.7021 - val_loss: 1.5746 - val_acc: 0.7170\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.5813 - acc: 0.7031 - val_loss: 1.5697 - val_acc: 0.7130\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5737 - acc: 0.7048 - val_loss: 1.5646 - val_acc: 0.7030\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5659 - acc: 0.7052 - val_loss: 1.5547 - val_acc: 0.7070\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5584 - acc: 0.7051 - val_loss: 1.5467 - val_acc: 0.7120\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5520 - acc: 0.7048 - val_loss: 1.5426 - val_acc: 0.7030\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5440 - acc: 0.7052 - val_loss: 1.5317 - val_acc: 0.7130\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.5370 - acc: 0.7064 - val_loss: 1.5255 - val_acc: 0.7070\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.5303 - acc: 0.7077 - val_loss: 1.5259 - val_acc: 0.7090\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5231 - acc: 0.7056 - val_loss: 1.5132 - val_acc: 0.7060\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5157 - acc: 0.7064 - val_loss: 1.5036 - val_acc: 0.7170\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5090 - acc: 0.7081 - val_loss: 1.5010 - val_acc: 0.7130\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5030 - acc: 0.7080 - val_loss: 1.4887 - val_acc: 0.7150\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4958 - acc: 0.7105 - val_loss: 1.4841 - val_acc: 0.7170\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4891 - acc: 0.7085 - val_loss: 1.4806 - val_acc: 0.7110\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4827 - acc: 0.7085 - val_loss: 1.4709 - val_acc: 0.7140\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4757 - acc: 0.7107 - val_loss: 1.4671 - val_acc: 0.7070\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4700 - acc: 0.7097 - val_loss: 1.4557 - val_acc: 0.7170\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4632 - acc: 0.7111 - val_loss: 1.4505 - val_acc: 0.7200\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4567 - acc: 0.7119 - val_loss: 1.4511 - val_acc: 0.7120\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4507 - acc: 0.7112 - val_loss: 1.4411 - val_acc: 0.7170\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4447 - acc: 0.7121 - val_loss: 1.4307 - val_acc: 0.7210\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4383 - acc: 0.7117 - val_loss: 1.4264 - val_acc: 0.7180\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4328 - acc: 0.7127 - val_loss: 1.4187 - val_acc: 0.7240\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4257 - acc: 0.7139 - val_loss: 1.4140 - val_acc: 0.7170\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4198 - acc: 0.7145 - val_loss: 1.4146 - val_acc: 0.7110\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4148 - acc: 0.7153 - val_loss: 1.4035 - val_acc: 0.7160\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4089 - acc: 0.7151 - val_loss: 1.4049 - val_acc: 0.7120\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.4031 - acc: 0.7168 - val_loss: 1.3912 - val_acc: 0.7140\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3970 - acc: 0.7160 - val_loss: 1.3837 - val_acc: 0.7230\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3913 - acc: 0.7165 - val_loss: 1.3815 - val_acc: 0.7180\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3857 - acc: 0.7172 - val_loss: 1.3785 - val_acc: 0.7170\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3801 - acc: 0.7177 - val_loss: 1.3696 - val_acc: 0.7190\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3747 - acc: 0.7179 - val_loss: 1.3609 - val_acc: 0.7210\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3687 - acc: 0.7180 - val_loss: 1.3592 - val_acc: 0.7190\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3636 - acc: 0.7189 - val_loss: 1.3552 - val_acc: 0.7210\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3587 - acc: 0.7188 - val_loss: 1.3471 - val_acc: 0.7220\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3543 - acc: 0.7197 - val_loss: 1.3407 - val_acc: 0.7240\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3481 - acc: 0.7200 - val_loss: 1.3418 - val_acc: 0.7160\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3433 - acc: 0.7196 - val_loss: 1.3327 - val_acc: 0.7220\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3378 - acc: 0.7215 - val_loss: 1.3257 - val_acc: 0.7260\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3329 - acc: 0.7207 - val_loss: 1.3204 - val_acc: 0.7230\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3283 - acc: 0.7215 - val_loss: 1.3173 - val_acc: 0.7190\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3232 - acc: 0.7237 - val_loss: 1.3200 - val_acc: 0.7200\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.3188 - acc: 0.7220 - val_loss: 1.3041 - val_acc: 0.7260\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3132 - acc: 0.7235 - val_loss: 1.3030 - val_acc: 0.7290\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3092 - acc: 0.7241 - val_loss: 1.3025 - val_acc: 0.7200\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.3046 - acc: 0.7224 - val_loss: 1.2919 - val_acc: 0.7300\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2991 - acc: 0.7236 - val_loss: 1.2861 - val_acc: 0.7320\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2944 - acc: 0.7257 - val_loss: 1.2816 - val_acc: 0.7290\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2902 - acc: 0.7265 - val_loss: 1.2780 - val_acc: 0.7270\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2859 - acc: 0.7255 - val_loss: 1.2773 - val_acc: 0.7240\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2815 - acc: 0.7275 - val_loss: 1.2704 - val_acc: 0.7300\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2766 - acc: 0.7277 - val_loss: 1.2657 - val_acc: 0.7290\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2726 - acc: 0.7253 - val_loss: 1.2640 - val_acc: 0.7280\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2684 - acc: 0.7264 - val_loss: 1.2599 - val_acc: 0.7230\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2638 - acc: 0.7279 - val_loss: 1.2520 - val_acc: 0.7330\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2600 - acc: 0.7287 - val_loss: 1.2499 - val_acc: 0.7330\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2560 - acc: 0.7291 - val_loss: 1.2467 - val_acc: 0.7310\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.2518 - acc: 0.7265 - val_loss: 1.2443 - val_acc: 0.7250\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2476 - acc: 0.7281 - val_loss: 1.2414 - val_acc: 0.7240\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2438 - acc: 0.7284 - val_loss: 1.2326 - val_acc: 0.7340\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2397 - acc: 0.7299 - val_loss: 1.2344 - val_acc: 0.7280\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2371 - acc: 0.7292 - val_loss: 1.2240 - val_acc: 0.7350\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2318 - acc: 0.7312 - val_loss: 1.2215 - val_acc: 0.7320\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2285 - acc: 0.7304 - val_loss: 1.2174 - val_acc: 0.7320\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2243 - acc: 0.7296 - val_loss: 1.2122 - val_acc: 0.7320\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2201 - acc: 0.7308 - val_loss: 1.2065 - val_acc: 0.7370\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2166 - acc: 0.7328 - val_loss: 1.2052 - val_acc: 0.7360\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2131 - acc: 0.7332 - val_loss: 1.2050 - val_acc: 0.7400\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.2096 - acc: 0.7341 - val_loss: 1.1995 - val_acc: 0.7370\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2055 - acc: 0.7323 - val_loss: 1.2025 - val_acc: 0.7310\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.2028 - acc: 0.7315 - val_loss: 1.1933 - val_acc: 0.7340\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1985 - acc: 0.7331 - val_loss: 1.1869 - val_acc: 0.7330\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1953 - acc: 0.7348 - val_loss: 1.1856 - val_acc: 0.7330\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1920 - acc: 0.7343 - val_loss: 1.1816 - val_acc: 0.7340\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1888 - acc: 0.7355 - val_loss: 1.1795 - val_acc: 0.7460\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1853 - acc: 0.7353 - val_loss: 1.1754 - val_acc: 0.7370\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1825 - acc: 0.7352 - val_loss: 1.1697 - val_acc: 0.7390\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1790 - acc: 0.7360 - val_loss: 1.1748 - val_acc: 0.7370\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1761 - acc: 0.7379 - val_loss: 1.1683 - val_acc: 0.7310\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1725 - acc: 0.7371 - val_loss: 1.1677 - val_acc: 0.7340\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1692 - acc: 0.7352 - val_loss: 1.1580 - val_acc: 0.7470\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1657 - acc: 0.7377 - val_loss: 1.1575 - val_acc: 0.7440\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1632 - acc: 0.7364 - val_loss: 1.1519 - val_acc: 0.7400\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1608 - acc: 0.7363 - val_loss: 1.1519 - val_acc: 0.7430\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1581 - acc: 0.7372 - val_loss: 1.1500 - val_acc: 0.7410\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1543 - acc: 0.7385 - val_loss: 1.1563 - val_acc: 0.7330\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1527 - acc: 0.7396 - val_loss: 1.1532 - val_acc: 0.7380\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1492 - acc: 0.7407 - val_loss: 1.1402 - val_acc: 0.7440\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1462 - acc: 0.7393 - val_loss: 1.1360 - val_acc: 0.7430\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1437 - acc: 0.7379 - val_loss: 1.1340 - val_acc: 0.7440\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1410 - acc: 0.7393 - val_loss: 1.1346 - val_acc: 0.7470\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1379 - acc: 0.7409 - val_loss: 1.1311 - val_acc: 0.7480\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1359 - acc: 0.7389 - val_loss: 1.1364 - val_acc: 0.7370\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1336 - acc: 0.7391 - val_loss: 1.1329 - val_acc: 0.7370\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1309 - acc: 0.7405 - val_loss: 1.1215 - val_acc: 0.7450\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1282 - acc: 0.7399 - val_loss: 1.1222 - val_acc: 0.7490\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1264 - acc: 0.7408 - val_loss: 1.1158 - val_acc: 0.7470\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1225 - acc: 0.7411 - val_loss: 1.1146 - val_acc: 0.7470\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1208 - acc: 0.7415 - val_loss: 1.1158 - val_acc: 0.7450\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.1191 - acc: 0.7419 - val_loss: 1.1092 - val_acc: 0.7450\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1167 - acc: 0.7417 - val_loss: 1.1075 - val_acc: 0.7480\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1144 - acc: 0.7413 - val_loss: 1.1059 - val_acc: 0.7470\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1125 - acc: 0.7393 - val_loss: 1.1027 - val_acc: 0.7500\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1102 - acc: 0.7432 - val_loss: 1.1031 - val_acc: 0.7470\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1079 - acc: 0.7419 - val_loss: 1.1001 - val_acc: 0.7460\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1053 - acc: 0.7436 - val_loss: 1.0979 - val_acc: 0.7430\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.1033 - acc: 0.7420 - val_loss: 1.0990 - val_acc: 0.7430\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.1017 - acc: 0.7425 - val_loss: 1.0945 - val_acc: 0.7470\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0993 - acc: 0.7433 - val_loss: 1.0955 - val_acc: 0.7430\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0978 - acc: 0.7456 - val_loss: 1.0904 - val_acc: 0.7490\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0956 - acc: 0.7436 - val_loss: 1.0900 - val_acc: 0.7500\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0937 - acc: 0.7453 - val_loss: 1.0884 - val_acc: 0.7480\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0915 - acc: 0.7431 - val_loss: 1.0847 - val_acc: 0.7490\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0896 - acc: 0.7453 - val_loss: 1.0851 - val_acc: 0.7480\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0878 - acc: 0.7461 - val_loss: 1.0794 - val_acc: 0.7510\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0858 - acc: 0.7444 - val_loss: 1.0851 - val_acc: 0.7490\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0839 - acc: 0.7449 - val_loss: 1.0758 - val_acc: 0.7520\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0816 - acc: 0.7459 - val_loss: 1.0757 - val_acc: 0.7480\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0805 - acc: 0.7459 - val_loss: 1.0711 - val_acc: 0.7520\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0782 - acc: 0.7463 - val_loss: 1.0737 - val_acc: 0.7500\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0765 - acc: 0.7461 - val_loss: 1.0855 - val_acc: 0.7440\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0759 - acc: 0.7460 - val_loss: 1.0684 - val_acc: 0.7490\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0739 - acc: 0.7449 - val_loss: 1.0654 - val_acc: 0.7520\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0714 - acc: 0.7460 - val_loss: 1.0711 - val_acc: 0.7470\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0696 - acc: 0.7503 - val_loss: 1.0688 - val_acc: 0.7470\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0681 - acc: 0.7467 - val_loss: 1.0636 - val_acc: 0.7540\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0670 - acc: 0.7461 - val_loss: 1.0600 - val_acc: 0.7490\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0649 - acc: 0.7473 - val_loss: 1.0572 - val_acc: 0.7520\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0634 - acc: 0.7479 - val_loss: 1.0567 - val_acc: 0.7510\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0619 - acc: 0.7491 - val_loss: 1.0575 - val_acc: 0.7530\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0603 - acc: 0.7484 - val_loss: 1.0538 - val_acc: 0.7530\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0589 - acc: 0.7489 - val_loss: 1.0652 - val_acc: 0.7480\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0573 - acc: 0.7476 - val_loss: 1.0600 - val_acc: 0.7450\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0557 - acc: 0.7489 - val_loss: 1.0511 - val_acc: 0.7520\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0545 - acc: 0.7480 - val_loss: 1.0473 - val_acc: 0.7530\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0523 - acc: 0.7483 - val_loss: 1.0483 - val_acc: 0.7540\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0514 - acc: 0.7500 - val_loss: 1.0438 - val_acc: 0.7520\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0493 - acc: 0.7501 - val_loss: 1.0430 - val_acc: 0.7530\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0480 - acc: 0.7504 - val_loss: 1.0422 - val_acc: 0.7600\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0460 - acc: 0.7493 - val_loss: 1.0397 - val_acc: 0.7600\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0455 - acc: 0.7507 - val_loss: 1.0403 - val_acc: 0.7550\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0435 - acc: 0.7496 - val_loss: 1.0394 - val_acc: 0.7520\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0420 - acc: 0.7523 - val_loss: 1.0384 - val_acc: 0.7540\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0417 - acc: 0.7489 - val_loss: 1.0436 - val_acc: 0.7480\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0402 - acc: 0.7507 - val_loss: 1.0336 - val_acc: 0.7550\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0377 - acc: 0.7507 - val_loss: 1.0374 - val_acc: 0.7530\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0377 - acc: 0.7496 - val_loss: 1.0327 - val_acc: 0.7550\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0360 - acc: 0.7513 - val_loss: 1.0290 - val_acc: 0.7530\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0337 - acc: 0.7513 - val_loss: 1.0371 - val_acc: 0.7540\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0329 - acc: 0.7523 - val_loss: 1.0270 - val_acc: 0.7550\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0317 - acc: 0.7521 - val_loss: 1.0322 - val_acc: 0.7540\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.0303 - acc: 0.7525 - val_loss: 1.0251 - val_acc: 0.7560\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0290 - acc: 0.7536 - val_loss: 1.0287 - val_acc: 0.7580\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0275 - acc: 0.7508 - val_loss: 1.0264 - val_acc: 0.7560\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0265 - acc: 0.7515 - val_loss: 1.0202 - val_acc: 0.7560\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0255 - acc: 0.7537 - val_loss: 1.0272 - val_acc: 0.7570\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0243 - acc: 0.7528 - val_loss: 1.0214 - val_acc: 0.7580\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0230 - acc: 0.7536 - val_loss: 1.0391 - val_acc: 0.7470\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0215 - acc: 0.7547 - val_loss: 1.0196 - val_acc: 0.7570\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0198 - acc: 0.7532 - val_loss: 1.0173 - val_acc: 0.7560\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0193 - acc: 0.7533 - val_loss: 1.0183 - val_acc: 0.7550\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0178 - acc: 0.7544 - val_loss: 1.0149 - val_acc: 0.7560\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0165 - acc: 0.7539 - val_loss: 1.0228 - val_acc: 0.7560\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0159 - acc: 0.7527 - val_loss: 1.0305 - val_acc: 0.7500\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0149 - acc: 0.7539 - val_loss: 1.0104 - val_acc: 0.7580\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0137 - acc: 0.7545 - val_loss: 1.0124 - val_acc: 0.7610\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0125 - acc: 0.7567 - val_loss: 1.0154 - val_acc: 0.7560\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0108 - acc: 0.7533 - val_loss: 1.0093 - val_acc: 0.7580\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0102 - acc: 0.7564 - val_loss: 1.0059 - val_acc: 0.7620\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0087 - acc: 0.7561 - val_loss: 1.0064 - val_acc: 0.7640\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0075 - acc: 0.7559 - val_loss: 1.0074 - val_acc: 0.7610\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0065 - acc: 0.7544 - val_loss: 1.0026 - val_acc: 0.7590\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0049 - acc: 0.7565 - val_loss: 1.0053 - val_acc: 0.7630\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0037 - acc: 0.7552 - val_loss: 1.0023 - val_acc: 0.7610\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0033 - acc: 0.7571 - val_loss: 1.0067 - val_acc: 0.7530\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0027 - acc: 0.7568 - val_loss: 1.0027 - val_acc: 0.7620\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 1.0013 - acc: 0.7583 - val_loss: 1.0114 - val_acc: 0.7560\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 1.0005 - acc: 0.7559 - val_loss: 0.9976 - val_acc: 0.7630\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9993 - acc: 0.7568 - val_loss: 0.9981 - val_acc: 0.7620\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9981 - acc: 0.7585 - val_loss: 0.9982 - val_acc: 0.7600\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9966 - acc: 0.7585 - val_loss: 0.9961 - val_acc: 0.7640\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9951 - acc: 0.7572 - val_loss: 0.9945 - val_acc: 0.7620\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9944 - acc: 0.7587 - val_loss: 0.9972 - val_acc: 0.7630\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9936 - acc: 0.7583 - val_loss: 1.0038 - val_acc: 0.7530\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9937 - acc: 0.7579 - val_loss: 0.9954 - val_acc: 0.7600\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9921 - acc: 0.7599 - val_loss: 0.9925 - val_acc: 0.7640\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9904 - acc: 0.7587 - val_loss: 0.9892 - val_acc: 0.7630\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9890 - acc: 0.7589 - val_loss: 0.9886 - val_acc: 0.7630\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9886 - acc: 0.7593 - val_loss: 0.9899 - val_acc: 0.7630\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9870 - acc: 0.7591 - val_loss: 0.9884 - val_acc: 0.7600\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9873 - acc: 0.7567 - val_loss: 0.9869 - val_acc: 0.7620\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9859 - acc: 0.7608 - val_loss: 0.9889 - val_acc: 0.7580\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9854 - acc: 0.7591 - val_loss: 0.9876 - val_acc: 0.7600\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9840 - acc: 0.7593 - val_loss: 0.9839 - val_acc: 0.7630\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9829 - acc: 0.7587 - val_loss: 0.9840 - val_acc: 0.7590\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9826 - acc: 0.7600 - val_loss: 0.9834 - val_acc: 0.7600\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9815 - acc: 0.7576 - val_loss: 0.9802 - val_acc: 0.7590\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9800 - acc: 0.7596 - val_loss: 0.9797 - val_acc: 0.7650\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9797 - acc: 0.7609 - val_loss: 0.9843 - val_acc: 0.7680\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9787 - acc: 0.7607 - val_loss: 0.9786 - val_acc: 0.7660\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9775 - acc: 0.7607 - val_loss: 0.9792 - val_acc: 0.7580\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9768 - acc: 0.7607 - val_loss: 0.9832 - val_acc: 0.7650\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9766 - acc: 0.7604 - val_loss: 0.9901 - val_acc: 0.7560\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9757 - acc: 0.7619 - val_loss: 0.9756 - val_acc: 0.7670\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9741 - acc: 0.7616 - val_loss: 0.9787 - val_acc: 0.7580\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9742 - acc: 0.7616 - val_loss: 0.9818 - val_acc: 0.7620\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9728 - acc: 0.7613 - val_loss: 0.9741 - val_acc: 0.7630\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9717 - acc: 0.7620 - val_loss: 0.9887 - val_acc: 0.7570\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9717 - acc: 0.7621 - val_loss: 0.9731 - val_acc: 0.7620\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9699 - acc: 0.7619 - val_loss: 0.9739 - val_acc: 0.7640\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9694 - acc: 0.7629 - val_loss: 0.9686 - val_acc: 0.7610\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9685 - acc: 0.7625 - val_loss: 0.9755 - val_acc: 0.7640\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9682 - acc: 0.7611 - val_loss: 0.9714 - val_acc: 0.7620\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9677 - acc: 0.7631 - val_loss: 0.9687 - val_acc: 0.7690\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9665 - acc: 0.7613 - val_loss: 0.9722 - val_acc: 0.7620\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9654 - acc: 0.7612 - val_loss: 0.9918 - val_acc: 0.7530\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9660 - acc: 0.7624 - val_loss: 0.9679 - val_acc: 0.7620\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9638 - acc: 0.7637 - val_loss: 0.9671 - val_acc: 0.7600\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9632 - acc: 0.7616 - val_loss: 0.9697 - val_acc: 0.7610\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9638 - acc: 0.7624 - val_loss: 0.9655 - val_acc: 0.7620\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9618 - acc: 0.7628 - val_loss: 0.9784 - val_acc: 0.7590\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9608 - acc: 0.7652 - val_loss: 0.9702 - val_acc: 0.7570\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9606 - acc: 0.7624 - val_loss: 0.9627 - val_acc: 0.7640\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9595 - acc: 0.7637 - val_loss: 0.9616 - val_acc: 0.7650\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9591 - acc: 0.7645 - val_loss: 0.9591 - val_acc: 0.7630\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9572 - acc: 0.7644 - val_loss: 0.9633 - val_acc: 0.7670\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9575 - acc: 0.7647 - val_loss: 0.9641 - val_acc: 0.7630\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9565 - acc: 0.7636 - val_loss: 0.9604 - val_acc: 0.7640\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9565 - acc: 0.7641 - val_loss: 0.9600 - val_acc: 0.7600\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9554 - acc: 0.7640 - val_loss: 0.9574 - val_acc: 0.7670\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9545 - acc: 0.7636 - val_loss: 0.9611 - val_acc: 0.7670\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9545 - acc: 0.7653 - val_loss: 0.9637 - val_acc: 0.7640\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9536 - acc: 0.7661 - val_loss: 0.9570 - val_acc: 0.7570\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9528 - acc: 0.7641 - val_loss: 0.9556 - val_acc: 0.7610\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9524 - acc: 0.7637 - val_loss: 0.9629 - val_acc: 0.7670\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9520 - acc: 0.7648 - val_loss: 0.9568 - val_acc: 0.7620\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9514 - acc: 0.7669 - val_loss: 0.9593 - val_acc: 0.7670\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9498 - acc: 0.7648 - val_loss: 0.9557 - val_acc: 0.7680\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9497 - acc: 0.7639 - val_loss: 0.9549 - val_acc: 0.7680\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9487 - acc: 0.7655 - val_loss: 0.9528 - val_acc: 0.7680\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9480 - acc: 0.7663 - val_loss: 0.9532 - val_acc: 0.7670\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9478 - acc: 0.7661 - val_loss: 0.9537 - val_acc: 0.7620\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9463 - acc: 0.7681 - val_loss: 0.9512 - val_acc: 0.7700\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9462 - acc: 0.7669 - val_loss: 0.9529 - val_acc: 0.7640\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9459 - acc: 0.7652 - val_loss: 0.9516 - val_acc: 0.7590\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9449 - acc: 0.7677 - val_loss: 0.9569 - val_acc: 0.7670\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9444 - acc: 0.7668 - val_loss: 0.9519 - val_acc: 0.7650\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9443 - acc: 0.7652 - val_loss: 0.9484 - val_acc: 0.7690\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9426 - acc: 0.7661 - val_loss: 0.9483 - val_acc: 0.7620\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9420 - acc: 0.7669 - val_loss: 0.9483 - val_acc: 0.7680\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9417 - acc: 0.7680 - val_loss: 0.9491 - val_acc: 0.7590\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9410 - acc: 0.7651 - val_loss: 0.9468 - val_acc: 0.7670\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9406 - acc: 0.7676 - val_loss: 0.9497 - val_acc: 0.7610\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9412 - acc: 0.7663 - val_loss: 0.9470 - val_acc: 0.7630\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9398 - acc: 0.7673 - val_loss: 0.9461 - val_acc: 0.7710\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9387 - acc: 0.7683 - val_loss: 0.9489 - val_acc: 0.7670\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9384 - acc: 0.7669 - val_loss: 0.9506 - val_acc: 0.7610\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9385 - acc: 0.7673 - val_loss: 0.9528 - val_acc: 0.7600\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9368 - acc: 0.7673 - val_loss: 0.9437 - val_acc: 0.7660\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9372 - acc: 0.7683 - val_loss: 0.9499 - val_acc: 0.7720\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9365 - acc: 0.7679 - val_loss: 0.9446 - val_acc: 0.7710\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9360 - acc: 0.7688 - val_loss: 0.9415 - val_acc: 0.7660\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9360 - acc: 0.7675 - val_loss: 0.9418 - val_acc: 0.7730\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9344 - acc: 0.7685 - val_loss: 0.9419 - val_acc: 0.7650\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9342 - acc: 0.7667 - val_loss: 0.9490 - val_acc: 0.7620\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9341 - acc: 0.7659 - val_loss: 0.9419 - val_acc: 0.7680\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9334 - acc: 0.7691 - val_loss: 0.9400 - val_acc: 0.7730\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9323 - acc: 0.7679 - val_loss: 0.9443 - val_acc: 0.7690\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9323 - acc: 0.7700 - val_loss: 0.9398 - val_acc: 0.7700\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9324 - acc: 0.7677 - val_loss: 0.9451 - val_acc: 0.7700\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9311 - acc: 0.7688 - val_loss: 0.9441 - val_acc: 0.7680\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9303 - acc: 0.7688 - val_loss: 0.9435 - val_acc: 0.7660\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9306 - acc: 0.7684 - val_loss: 0.9405 - val_acc: 0.7730\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9296 - acc: 0.7691 - val_loss: 0.9402 - val_acc: 0.7720\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9284 - acc: 0.7705 - val_loss: 0.9494 - val_acc: 0.7640\n",

      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9292 - acc: 0.7707 - val_loss: 0.9412 - val_acc: 0.7720\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9289 - acc: 0.7715 - val_loss: 0.9393 - val_acc: 0.7720\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9278 - acc: 0.7692 - val_loss: 0.9440 - val_acc: 0.7690\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9273 - acc: 0.7701 - val_loss: 0.9396 - val_acc: 0.7650\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9270 - acc: 0.7685 - val_loss: 0.9472 - val_acc: 0.7700\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9270 - acc: 0.7697 - val_loss: 0.9343 - val_acc: 0.7740\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9257 - acc: 0.7715 - val_loss: 0.9328 - val_acc: 0.7630\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9259 - acc: 0.7701 - val_loss: 0.9351 - val_acc: 0.7740\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9246 - acc: 0.7696 - val_loss: 0.9376 - val_acc: 0.7710\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9239 - acc: 0.7707 - val_loss: 0.9339 - val_acc: 0.7720\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9240 - acc: 0.7699 - val_loss: 0.9311 - val_acc: 0.7700\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9239 - acc: 0.7697 - val_loss: 0.9398 - val_acc: 0.7720\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9234 - acc: 0.7704 - val_loss: 0.9327 - val_acc: 0.7640\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9226 - acc: 0.7721 - val_loss: 0.9372 - val_acc: 0.7720\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9223 - acc: 0.7731 - val_loss: 0.9307 - val_acc: 0.7640\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9218 - acc: 0.7727 - val_loss: 0.9401 - val_acc: 0.7660\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9219 - acc: 0.7715 - val_loss: 0.9396 - val_acc: 0.7620\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9216 - acc: 0.7688 - val_loss: 0.9374 - val_acc: 0.7700\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9213 - acc: 0.7711 - val_loss: 0.9298 - val_acc: 0.7630\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9198 - acc: 0.7715 - val_loss: 0.9295 - val_acc: 0.7670\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9198 - acc: 0.7717 - val_loss: 0.9290 - val_acc: 0.7650\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9194 - acc: 0.7716 - val_loss: 0.9298 - val_acc: 0.7680\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9185 - acc: 0.7711 - val_loss: 0.9410 - val_acc: 0.7670\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9179 - acc: 0.7707 - val_loss: 0.9273 - val_acc: 0.7650\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9176 - acc: 0.7717 - val_loss: 0.9297 - val_acc: 0.7630\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9179 - acc: 0.7712 - val_loss: 0.9287 - val_acc: 0.7680\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9187 - acc: 0.7695 - val_loss: 0.9354 - val_acc: 0.7740\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9173 - acc: 0.7728 - val_loss: 0.9331 - val_acc: 0.7730\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9172 - acc: 0.7707 - val_loss: 0.9320 - val_acc: 0.7710\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9167 - acc: 0.7724 - val_loss: 0.9331 - val_acc: 0.7650\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9166 - acc: 0.7716 - val_loss: 0.9301 - val_acc: 0.7710\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9158 - acc: 0.7721 - val_loss: 0.9262 - val_acc: 0.7710\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9150 - acc: 0.7704 - val_loss: 0.9355 - val_acc: 0.7700\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9148 - acc: 0.7724 - val_loss: 0.9270 - val_acc: 0.7660\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9142 - acc: 0.7729 - val_loss: 0.9323 - val_acc: 0.7700\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9143 - acc: 0.7733 - val_loss: 0.9344 - val_acc: 0.7660\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9143 - acc: 0.7724 - val_loss: 0.9292 - val_acc: 0.7720\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9130 - acc: 0.7727 - val_loss: 0.9272 - val_acc: 0.7690\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9131 - acc: 0.7733 - val_loss: 0.9253 - val_acc: 0.7740\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9120 - acc: 0.7748 - val_loss: 0.9266 - val_acc: 0.7710\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9126 - acc: 0.7735 - val_loss: 0.9259 - val_acc: 0.7650\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9119 - acc: 0.7735 - val_loss: 0.9309 - val_acc: 0.7660\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9113 - acc: 0.7732 - val_loss: 0.9246 - val_acc: 0.7740\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9115 - acc: 0.7729 - val_loss: 0.9276 - val_acc: 0.7710\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9112 - acc: 0.7736 - val_loss: 0.9248 - val_acc: 0.7710\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9101 - acc: 0.7729 - val_loss: 0.9356 - val_acc: 0.7670\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9103 - acc: 0.7721 - val_loss: 0.9216 - val_acc: 0.7660\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9089 - acc: 0.7767 - val_loss: 0.9225 - val_acc: 0.7720\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9092 - acc: 0.7727 - val_loss: 0.9223 - val_acc: 0.7680\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9102 - acc: 0.7737 - val_loss: 0.9224 - val_acc: 0.7710\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9094 - acc: 0.7741 - val_loss: 0.9239 - val_acc: 0.7750\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9088 - acc: 0.7724 - val_loss: 0.9224 - val_acc: 0.7750\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9085 - acc: 0.7749 - val_loss: 0.9225 - val_acc: 0.7710\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9076 - acc: 0.7743 - val_loss: 0.9298 - val_acc: 0.7680\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9072 - acc: 0.7748 - val_loss: 0.9198 - val_acc: 0.7710\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9072 - acc: 0.7744 - val_loss: 0.9191 - val_acc: 0.7670\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9070 - acc: 0.7752 - val_loss: 0.9188 - val_acc: 0.7720\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9065 - acc: 0.7747 - val_loss: 0.9205 - val_acc: 0.7730\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9058 - acc: 0.7773 - val_loss: 0.9232 - val_acc: 0.7740\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9056 - acc: 0.7747 - val_loss: 0.9287 - val_acc: 0.7720\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9065 - acc: 0.7763 - val_loss: 0.9195 - val_acc: 0.7710\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9052 - acc: 0.7752 - val_loss: 0.9202 - val_acc: 0.7700\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9049 - acc: 0.7751 - val_loss: 0.9269 - val_acc: 0.7700\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9047 - acc: 0.7736 - val_loss: 0.9200 - val_acc: 0.7760\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9041 - acc: 0.7752 - val_loss: 0.9197 - val_acc: 0.7700\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9041 - acc: 0.7757 - val_loss: 0.9174 - val_acc: 0.7690\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9032 - acc: 0.7751 - val_loss: 0.9225 - val_acc: 0.7720\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9038 - acc: 0.7765 - val_loss: 0.9167 - val_acc: 0.7710\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9029 - acc: 0.7755 - val_loss: 0.9268 - val_acc: 0.7710\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9038 - acc: 0.7745 - val_loss: 0.9204 - val_acc: 0.7720\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9022 - acc: 0.7761 - val_loss: 0.9179 - val_acc: 0.7800\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.9025 - acc: 0.7755 - val_loss: 0.9194 - val_acc: 0.7710\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9011 - acc: 0.7768 - val_loss: 0.9236 - val_acc: 0.7650\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9013 - acc: 0.7759 - val_loss: 0.9173 - val_acc: 0.7720\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9015 - acc: 0.7751 - val_loss: 0.9210 - val_acc: 0.7750\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9005 - acc: 0.7776 - val_loss: 0.9269 - val_acc: 0.7630\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9010 - acc: 0.7761 - val_loss: 0.9371 - val_acc: 0.7670\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9009 - acc: 0.7763 - val_loss: 0.9224 - val_acc: 0.7720\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8995 - acc: 0.7771 - val_loss: 0.9196 - val_acc: 0.7720\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.9000 - acc: 0.7763 - val_loss: 0.9126 - val_acc: 0.7790\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8989 - acc: 0.7765 - val_loss: 0.9145 - val_acc: 0.7690\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8985 - acc: 0.7776 - val_loss: 0.9140 - val_acc: 0.7790\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8986 - acc: 0.7772 - val_loss: 0.9158 - val_acc: 0.7740\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8995 - acc: 0.7764 - val_loss: 0.9126 - val_acc: 0.7730\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8985 - acc: 0.7771 - val_loss: 0.9127 - val_acc: 0.7720\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8977 - acc: 0.7784 - val_loss: 0.9107 - val_acc: 0.7730\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8977 - acc: 0.7760 - val_loss: 0.9141 - val_acc: 0.7750\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8973 - acc: 0.7748 - val_loss: 0.9217 - val_acc: 0.7700\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8973 - acc: 0.7771 - val_loss: 0.9186 - val_acc: 0.7730\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8969 - acc: 0.7773 - val_loss: 0.9225 - val_acc: 0.7680\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8967 - acc: 0.7768 - val_loss: 0.9177 - val_acc: 0.7760\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8965 - acc: 0.7772 - val_loss: 0.9176 - val_acc: 0.7790\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8953 - acc: 0.7772 - val_loss: 0.9173 - val_acc: 0.7760\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8953 - acc: 0.7767 - val_loss: 0.9168 - val_acc: 0.7730\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8957 - acc: 0.7781 - val_loss: 0.9148 - val_acc: 0.7750\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8952 - acc: 0.7765 - val_loss: 0.9094 - val_acc: 0.7750\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8950 - acc: 0.7771 - val_loss: 0.9173 - val_acc: 0.7740\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8943 - acc: 0.7764 - val_loss: 0.9281 - val_acc: 0.7660\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8944 - acc: 0.7772 - val_loss: 0.9163 - val_acc: 0.7730\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8947 - acc: 0.7775 - val_loss: 0.9112 - val_acc: 0.7780\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8942 - acc: 0.7773 - val_loss: 0.9125 - val_acc: 0.7760\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8934 - acc: 0.7777 - val_loss: 0.9241 - val_acc: 0.7640\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8936 - acc: 0.7764 - val_loss: 0.9086 - val_acc: 0.7750\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8928 - acc: 0.7783 - val_loss: 0.9235 - val_acc: 0.7700\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8935 - acc: 0.7776 - val_loss: 0.9136 - val_acc: 0.7700\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8922 - acc: 0.7777 - val_loss: 0.9134 - val_acc: 0.7770\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8926 - acc: 0.7772 - val_loss: 0.9134 - val_acc: 0.7750\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8919 - acc: 0.7781 - val_loss: 0.9088 - val_acc: 0.7810\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8915 - acc: 0.7772 - val_loss: 0.9135 - val_acc: 0.7730\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8925 - acc: 0.7787 - val_loss: 0.9088 - val_acc: 0.7710\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8914 - acc: 0.7791 - val_loss: 0.9067 - val_acc: 0.7780\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8906 - acc: 0.7803 - val_loss: 0.9100 - val_acc: 0.7760\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8902 - acc: 0.7791 - val_loss: 0.9154 - val_acc: 0.7740\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8902 - acc: 0.7793 - val_loss: 0.9134 - val_acc: 0.7770\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8902 - acc: 0.7784 - val_loss: 0.9084 - val_acc: 0.7770\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8897 - acc: 0.7785 - val_loss: 0.9109 - val_acc: 0.7780\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8898 - acc: 0.7768 - val_loss: 0.9062 - val_acc: 0.7780\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8892 - acc: 0.7776 - val_loss: 0.9105 - val_acc: 0.7760\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8892 - acc: 0.7789 - val_loss: 0.9085 - val_acc: 0.7750\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8891 - acc: 0.7773 - val_loss: 0.9071 - val_acc: 0.7810\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8890 - acc: 0.7784 - val_loss: 0.9104 - val_acc: 0.7740\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8885 - acc: 0.7791 - val_loss: 0.9128 - val_acc: 0.7740\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8873 - acc: 0.7775 - val_loss: 0.9111 - val_acc: 0.7770\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8883 - acc: 0.7800 - val_loss: 0.9066 - val_acc: 0.7770\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8868 - acc: 0.7787 - val_loss: 0.9092 - val_acc: 0.7790\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8877 - acc: 0.7801 - val_loss: 0.9089 - val_acc: 0.7780\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8874 - acc: 0.7781 - val_loss: 0.9098 - val_acc: 0.7700\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8869 - acc: 0.7783 - val_loss: 0.9088 - val_acc: 0.7760\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8866 - acc: 0.7789 - val_loss: 0.9089 - val_acc: 0.7730\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8872 - acc: 0.7815 - val_loss: 0.9074 - val_acc: 0.7770\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8862 - acc: 0.7787 - val_loss: 0.9110 - val_acc: 0.7740\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8854 - acc: 0.7809 - val_loss: 0.9125 - val_acc: 0.7770\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8856 - acc: 0.7804 - val_loss: 0.9160 - val_acc: 0.7720\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8878 - acc: 0.7777 - val_loss: 0.9062 - val_acc: 0.7740\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8854 - acc: 0.7791 - val_loss: 0.9137 - val_acc: 0.7790\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8855 - acc: 0.7803 - val_loss: 0.9116 - val_acc: 0.7730\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8851 - acc: 0.7791 - val_loss: 0.9106 - val_acc: 0.7780\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8847 - acc: 0.7811 - val_loss: 0.9119 - val_acc: 0.7720\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8840 - acc: 0.7800 - val_loss: 0.9028 - val_acc: 0.7770\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8834 - acc: 0.7821 - val_loss: 0.9034 - val_acc: 0.7800\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8848 - acc: 0.7791 - val_loss: 0.9047 - val_acc: 0.7810\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8841 - acc: 0.7809 - val_loss: 0.9021 - val_acc: 0.7790\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8826 - acc: 0.7799 - val_loss: 0.9032 - val_acc: 0.7780\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8838 - acc: 0.7813 - val_loss: 0.9172 - val_acc: 0.7660\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8843 - acc: 0.7796 - val_loss: 0.9256 - val_acc: 0.7660\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8834 - acc: 0.7784 - val_loss: 0.9039 - val_acc: 0.7820\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8830 - acc: 0.7812 - val_loss: 0.9023 - val_acc: 0.7770\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8822 - acc: 0.7800 - val_loss: 0.9023 - val_acc: 0.7750\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8824 - acc: 0.7803 - val_loss: 0.9060 - val_acc: 0.7850\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8822 - acc: 0.7803 - val_loss: 0.9045 - val_acc: 0.7780\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8824 - acc: 0.7809 - val_loss: 0.9070 - val_acc: 0.7770\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8818 - acc: 0.7816 - val_loss: 0.9034 - val_acc: 0.7800\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8812 - acc: 0.7805 - val_loss: 0.9056 - val_acc: 0.7750\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8828 - acc: 0.7795 - val_loss: 0.9015 - val_acc: 0.7820\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8801 - acc: 0.7805 - val_loss: 0.9014 - val_acc: 0.7820\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8809 - acc: 0.7813 - val_loss: 0.9034 - val_acc: 0.7810\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8803 - acc: 0.7807 - val_loss: 0.9022 - val_acc: 0.7800\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8787 - acc: 0.7821 - val_loss: 0.9050 - val_acc: 0.7770\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8797 - acc: 0.7801 - val_loss: 0.8998 - val_acc: 0.7770\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8797 - acc: 0.7821 - val_loss: 0.9039 - val_acc: 0.7770\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8797 - acc: 0.7807 - val_loss: 0.9065 - val_acc: 0.7710\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8787 - acc: 0.7817 - val_loss: 0.9016 - val_acc: 0.7800\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8788 - acc: 0.7803 - val_loss: 0.9023 - val_acc: 0.7820\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8783 - acc: 0.7831 - val_loss: 0.9020 - val_acc: 0.7760\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8788 - acc: 0.7813 - val_loss: 0.8999 - val_acc: 0.7850\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8791 - acc: 0.7812 - val_loss: 0.8994 - val_acc: 0.7760\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8780 - acc: 0.7796 - val_loss: 0.9093 - val_acc: 0.7760\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8783 - acc: 0.7804 - val_loss: 0.8990 - val_acc: 0.7760\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8773 - acc: 0.7839 - val_loss: 0.9015 - val_acc: 0.7790\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8780 - acc: 0.7809 - val_loss: 0.8988 - val_acc: 0.7810\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8777 - acc: 0.7815 - val_loss: 0.9027 - val_acc: 0.7810\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8777 - acc: 0.7792 - val_loss: 0.9067 - val_acc: 0.7800\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8768 - acc: 0.7827 - val_loss: 0.8967 - val_acc: 0.7800\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8766 - acc: 0.7819 - val_loss: 0.8988 - val_acc: 0.7830\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8764 - acc: 0.7820 - val_loss: 0.9031 - val_acc: 0.7780\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8765 - acc: 0.7829 - val_loss: 0.9000 - val_acc: 0.7830\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8765 - acc: 0.7816 - val_loss: 0.8981 - val_acc: 0.7790\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8759 - acc: 0.7812 - val_loss: 0.9062 - val_acc: 0.7750\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8764 - acc: 0.7808 - val_loss: 0.8978 - val_acc: 0.7790\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8762 - acc: 0.7807 - val_loss: 0.9009 - val_acc: 0.7800\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8757 - acc: 0.7803 - val_loss: 0.9080 - val_acc: 0.7720\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8760 - acc: 0.7807 - val_loss: 0.8994 - val_acc: 0.7810\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8759 - acc: 0.7817 - val_loss: 0.8997 - val_acc: 0.7800\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8753 - acc: 0.7811 - val_loss: 0.9058 - val_acc: 0.7770\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8749 - acc: 0.7809 - val_loss: 0.8981 - val_acc: 0.7750\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8743 - acc: 0.7817 - val_loss: 0.9028 - val_acc: 0.7870\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8754 - acc: 0.7823 - val_loss: 0.9013 - val_acc: 0.7730\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8753 - acc: 0.7811 - val_loss: 0.9057 - val_acc: 0.7710\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8745 - acc: 0.7823 - val_loss: 0.9219 - val_acc: 0.7660\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8751 - acc: 0.7827 - val_loss: 0.9069 - val_acc: 0.7790\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8742 - acc: 0.7819 - val_loss: 0.9011 - val_acc: 0.7770\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8731 - acc: 0.7837 - val_loss: 0.8947 - val_acc: 0.7810\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8728 - acc: 0.7819 - val_loss: 0.9043 - val_acc: 0.7790\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8733 - acc: 0.7823 - val_loss: 0.8976 - val_acc: 0.7810\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8729 - acc: 0.7828 - val_loss: 0.9009 - val_acc: 0.7780\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8736 - acc: 0.7824 - val_loss: 0.9133 - val_acc: 0.7700\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8727 - acc: 0.7820 - val_loss: 0.8998 - val_acc: 0.7800\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8730 - acc: 0.7833 - val_loss: 0.9000 - val_acc: 0.7740\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8722 - acc: 0.7833 - val_loss: 0.8965 - val_acc: 0.7770\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8721 - acc: 0.7836 - val_loss: 0.8960 - val_acc: 0.7820\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8716 - acc: 0.7832 - val_loss: 0.8994 - val_acc: 0.7800\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8721 - acc: 0.7832 - val_loss: 0.8957 - val_acc: 0.7760\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8717 - acc: 0.7828 - val_loss: 0.8962 - val_acc: 0.7820\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8715 - acc: 0.7819 - val_loss: 0.8948 - val_acc: 0.7820\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8709 - acc: 0.7844 - val_loss: 0.8958 - val_acc: 0.7810\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8711 - acc: 0.7841 - val_loss: 0.9049 - val_acc: 0.7780\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8715 - acc: 0.7839 - val_loss: 0.8975 - val_acc: 0.7790\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8709 - acc: 0.7852 - val_loss: 0.8965 - val_acc: 0.7770\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8716 - acc: 0.7819 - val_loss: 0.8926 - val_acc: 0.7780\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8695 - acc: 0.7829 - val_loss: 0.9166 - val_acc: 0.7640\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8704 - acc: 0.7825 - val_loss: 0.9051 - val_acc: 0.7790\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8702 - acc: 0.7837 - val_loss: 0.9099 - val_acc: 0.7740\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8707 - acc: 0.7857 - val_loss: 0.9061 - val_acc: 0.7720\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8696 - acc: 0.7841 - val_loss: 0.9031 - val_acc: 0.7700\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8696 - acc: 0.7840 - val_loss: 0.9015 - val_acc: 0.7800\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8695 - acc: 0.7847 - val_loss: 0.8986 - val_acc: 0.7800\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8685 - acc: 0.7845 - val_loss: 0.8974 - val_acc: 0.7760\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8687 - acc: 0.7859 - val_loss: 0.8997 - val_acc: 0.7780\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8688 - acc: 0.7832 - val_loss: 0.8989 - val_acc: 0.7760\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8679 - acc: 0.7840 - val_loss: 0.8931 - val_acc: 0.7830\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8674 - acc: 0.7857 - val_loss: 0.9107 - val_acc: 0.7790\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8688 - acc: 0.7841 - val_loss: 0.8937 - val_acc: 0.7810\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8673 - acc: 0.7860 - val_loss: 0.8980 - val_acc: 0.7800\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8674 - acc: 0.7809 - val_loss: 0.9000 - val_acc: 0.7740\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8672 - acc: 0.7832 - val_loss: 0.9121 - val_acc: 0.7680\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8676 - acc: 0.7843 - val_loss: 0.8957 - val_acc: 0.7720\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8681 - acc: 0.7828 - val_loss: 0.9033 - val_acc: 0.7750\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8674 - acc: 0.7833 - val_loss: 0.8942 - val_acc: 0.7850\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8671 - acc: 0.7819 - val_loss: 0.8909 - val_acc: 0.7760\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8665 - acc: 0.7836 - val_loss: 0.8946 - val_acc: 0.7820\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8670 - acc: 0.7831 - val_loss: 0.8958 - val_acc: 0.7770\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8651 - acc: 0.7851 - val_loss: 0.8940 - val_acc: 0.7790\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8669 - acc: 0.7853 - val_loss: 0.8978 - val_acc: 0.7790\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8659 - acc: 0.7847 - val_loss: 0.8908 - val_acc: 0.7870\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8658 - acc: 0.7855 - val_loss: 0.8894 - val_acc: 0.7840\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8659 - acc: 0.7877 - val_loss: 0.8921 - val_acc: 0.7810\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8653 - acc: 0.7856 - val_loss: 0.8913 - val_acc: 0.7820\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8645 - acc: 0.7856 - val_loss: 0.8931 - val_acc: 0.7840\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8657 - acc: 0.7829 - val_loss: 0.8915 - val_acc: 0.7800\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8645 - acc: 0.7836 - val_loss: 0.9091 - val_acc: 0.7750\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8644 - acc: 0.7851 - val_loss: 0.8968 - val_acc: 0.7780\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8641 - acc: 0.7864 - val_loss: 0.9016 - val_acc: 0.7730\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8653 - acc: 0.7840 - val_loss: 0.8950 - val_acc: 0.7780\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8640 - acc: 0.7859 - val_loss: 0.9015 - val_acc: 0.7740\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8643 - acc: 0.7841 - val_loss: 0.9000 - val_acc: 0.7780\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8634 - acc: 0.7863 - val_loss: 0.8929 - val_acc: 0.7800\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8632 - acc: 0.7849 - val_loss: 0.8987 - val_acc: 0.7790\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8631 - acc: 0.7860 - val_loss: 0.9312 - val_acc: 0.7720\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8657 - acc: 0.7833 - val_loss: 0.8910 - val_acc: 0.7830\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8633 - acc: 0.7857 - val_loss: 0.8905 - val_acc: 0.7800\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8637 - acc: 0.7836 - val_loss: 0.8922 - val_acc: 0.7800\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8623 - acc: 0.7851 - val_loss: 0.8882 - val_acc: 0.7850\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8614 - acc: 0.7863 - val_loss: 0.9030 - val_acc: 0.7690\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8629 - acc: 0.7871 - val_loss: 0.8911 - val_acc: 0.7790\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8617 - acc: 0.7845 - val_loss: 0.8902 - val_acc: 0.7800\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8628 - acc: 0.7871 - val_loss: 0.9009 - val_acc: 0.7730\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8629 - acc: 0.7857 - val_loss: 0.8952 - val_acc: 0.7730\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8606 - acc: 0.7889 - val_loss: 0.8938 - val_acc: 0.7740\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8619 - acc: 0.7845 - val_loss: 0.8887 - val_acc: 0.7780\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8616 - acc: 0.7863 - val_loss: 0.8933 - val_acc: 0.7830\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8609 - acc: 0.7871 - val_loss: 0.8915 - val_acc: 0.7850\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8620 - acc: 0.7859 - val_loss: 0.8892 - val_acc: 0.7820\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8615 - acc: 0.7852 - val_loss: 0.8955 - val_acc: 0.7830\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8623 - acc: 0.7855 - val_loss: 0.8973 - val_acc: 0.7770\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8614 - acc: 0.7856 - val_loss: 0.8917 - val_acc: 0.7840\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8603 - acc: 0.7837 - val_loss: 0.8890 - val_acc: 0.7810\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8601 - acc: 0.7873 - val_loss: 0.8904 - val_acc: 0.7840\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8607 - acc: 0.7876 - val_loss: 0.8904 - val_acc: 0.7840\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8588 - acc: 0.7872 - val_loss: 0.8983 - val_acc: 0.7820\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8608 - acc: 0.7869 - val_loss: 0.9171 - val_acc: 0.7730\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8610 - acc: 0.7864 - val_loss: 0.8994 - val_acc: 0.7790\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8605 - acc: 0.7871 - val_loss: 0.8894 - val_acc: 0.7750\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8601 - acc: 0.7868 - val_loss: 0.8948 - val_acc: 0.7750\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8593 - acc: 0.7891 - val_loss: 0.8867 - val_acc: 0.7850\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8596 - acc: 0.7864 - val_loss: 0.8927 - val_acc: 0.7800\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8605 - acc: 0.7852 - val_loss: 0.9066 - val_acc: 0.7750\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8609 - acc: 0.7868 - val_loss: 0.8944 - val_acc: 0.7700\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8582 - acc: 0.7873 - val_loss: 0.8936 - val_acc: 0.7780\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8579 - acc: 0.7881 - val_loss: 0.8971 - val_acc: 0.7800\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8594 - acc: 0.7859 - val_loss: 0.8901 - val_acc: 0.7800\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8585 - acc: 0.7884 - val_loss: 0.8936 - val_acc: 0.7800\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8578 - acc: 0.7859 - val_loss: 0.8887 - val_acc: 0.7810\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8576 - acc: 0.7860 - val_loss: 0.8881 - val_acc: 0.7790\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8575 - acc: 0.7887 - val_loss: 0.9072 - val_acc: 0.7710\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8570 - acc: 0.7867 - val_loss: 0.8917 - val_acc: 0.7780\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8567 - acc: 0.7881 - val_loss: 0.8924 - val_acc: 0.7780\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8570 - acc: 0.7879 - val_loss: 0.8887 - val_acc: 0.7810\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8572 - acc: 0.7871 - val_loss: 0.8880 - val_acc: 0.7830\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8567 - acc: 0.7896 - val_loss: 0.8987 - val_acc: 0.7750\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8567 - acc: 0.7872 - val_loss: 0.8876 - val_acc: 0.7850\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8554 - acc: 0.7884 - val_loss: 0.8857 - val_acc: 0.7830\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8559 - acc: 0.7863 - val_loss: 0.8885 - val_acc: 0.7840\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8561 - acc: 0.7865 - val_loss: 0.8869 - val_acc: 0.7800\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8557 - acc: 0.7869 - val_loss: 0.9200 - val_acc: 0.7700\n",
      "Epoch 649/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8566 - acc: 0.7884 - val_loss: 0.8865 - val_acc: 0.7820\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8545 - acc: 0.7891 - val_loss: 0.8906 - val_acc: 0.7780\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8557 - acc: 0.7880 - val_loss: 0.8919 - val_acc: 0.7760\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8549 - acc: 0.7873 - val_loss: 0.8920 - val_acc: 0.7740\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8556 - acc: 0.7892 - val_loss: 0.8908 - val_acc: 0.7760\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8550 - acc: 0.7877 - val_loss: 0.8853 - val_acc: 0.7820\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8544 - acc: 0.7884 - val_loss: 0.8944 - val_acc: 0.7690\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8543 - acc: 0.7885 - val_loss: 0.8871 - val_acc: 0.7830\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8539 - acc: 0.7872 - val_loss: 0.8897 - val_acc: 0.7800\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8540 - acc: 0.7900 - val_loss: 0.8988 - val_acc: 0.7670\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8555 - acc: 0.7887 - val_loss: 0.9005 - val_acc: 0.7770\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8540 - acc: 0.7883 - val_loss: 0.8928 - val_acc: 0.7810\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8544 - acc: 0.7877 - val_loss: 0.8958 - val_acc: 0.7790\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8541 - acc: 0.7884 - val_loss: 0.8884 - val_acc: 0.7830\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8518 - acc: 0.7875 - val_loss: 0.8926 - val_acc: 0.7810\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8531 - acc: 0.7901 - val_loss: 0.8930 - val_acc: 0.7750\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8522 - acc: 0.7919 - val_loss: 0.8877 - val_acc: 0.7810\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8537 - acc: 0.7887 - val_loss: 0.8865 - val_acc: 0.7830\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8529 - acc: 0.7885 - val_loss: 0.8990 - val_acc: 0.7760\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8541 - acc: 0.7883 - val_loss: 0.8852 - val_acc: 0.7810\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8532 - acc: 0.7900 - val_loss: 0.8875 - val_acc: 0.7800\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8538 - acc: 0.7887 - val_loss: 0.8908 - val_acc: 0.7890\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8525 - acc: 0.7915 - val_loss: 0.8951 - val_acc: 0.7810\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8522 - acc: 0.7897 - val_loss: 0.8871 - val_acc: 0.7730\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8519 - acc: 0.7887 - val_loss: 0.8877 - val_acc: 0.7850\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8526 - acc: 0.7895 - val_loss: 0.8916 - val_acc: 0.7800\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8539 - acc: 0.7875 - val_loss: 0.8922 - val_acc: 0.7770\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8516 - acc: 0.7908 - val_loss: 0.8895 - val_acc: 0.7780\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8522 - acc: 0.7919 - val_loss: 0.8866 - val_acc: 0.7850\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8513 - acc: 0.7889 - val_loss: 0.8893 - val_acc: 0.7790\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8512 - acc: 0.7905 - val_loss: 0.8874 - val_acc: 0.7780\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8513 - acc: 0.7895 - val_loss: 0.8935 - val_acc: 0.7750\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8511 - acc: 0.7888 - val_loss: 0.8870 - val_acc: 0.7810\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8499 - acc: 0.7900 - val_loss: 0.8960 - val_acc: 0.7720\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8512 - acc: 0.7891 - val_loss: 0.8877 - val_acc: 0.7790\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8506 - acc: 0.7905 - val_loss: 0.8858 - val_acc: 0.7810\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8498 - acc: 0.7911 - val_loss: 0.8863 - val_acc: 0.7800\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8500 - acc: 0.7911 - val_loss: 0.8860 - val_acc: 0.7820\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8509 - acc: 0.7916 - val_loss: 0.8931 - val_acc: 0.7810\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8504 - acc: 0.7905 - val_loss: 0.8849 - val_acc: 0.7830\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8496 - acc: 0.7899 - val_loss: 0.8827 - val_acc: 0.7830\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8483 - acc: 0.7908 - val_loss: 0.8867 - val_acc: 0.7690\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8492 - acc: 0.7889 - val_loss: 0.8939 - val_acc: 0.7730\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8497 - acc: 0.7913 - val_loss: 0.8981 - val_acc: 0.7790\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8504 - acc: 0.7892 - val_loss: 0.8831 - val_acc: 0.7840\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8483 - acc: 0.7905 - val_loss: 0.8849 - val_acc: 0.7840\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8484 - acc: 0.7908 - val_loss: 0.9029 - val_acc: 0.7790\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8501 - acc: 0.7900 - val_loss: 0.8844 - val_acc: 0.7810\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8495 - acc: 0.7901 - val_loss: 0.8915 - val_acc: 0.7800\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8481 - acc: 0.7905 - val_loss: 0.8845 - val_acc: 0.7750\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8478 - acc: 0.7909 - val_loss: 0.8907 - val_acc: 0.7800\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8471 - acc: 0.7901 - val_loss: 0.8913 - val_acc: 0.7720\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8490 - acc: 0.7896 - val_loss: 0.8884 - val_acc: 0.7820\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8485 - acc: 0.7912 - val_loss: 0.9003 - val_acc: 0.7720\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8498 - acc: 0.7925 - val_loss: 0.8983 - val_acc: 0.7740\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8490 - acc: 0.7904 - val_loss: 0.8937 - val_acc: 0.7770\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8477 - acc: 0.7931 - val_loss: 0.8830 - val_acc: 0.7830\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8463 - acc: 0.7915 - val_loss: 0.9006 - val_acc: 0.7740\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8479 - acc: 0.7901 - val_loss: 0.8928 - val_acc: 0.7740\n",
      "Epoch 708/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8473 - acc: 0.7919 - val_loss: 0.9070 - val_acc: 0.7720\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8472 - acc: 0.7923 - val_loss: 0.8840 - val_acc: 0.7820\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8462 - acc: 0.7931 - val_loss: 0.8816 - val_acc: 0.7820\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8466 - acc: 0.7916 - val_loss: 0.8936 - val_acc: 0.7770\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8466 - acc: 0.7932 - val_loss: 0.8888 - val_acc: 0.7820\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8460 - acc: 0.7907 - val_loss: 0.8844 - val_acc: 0.7860\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8462 - acc: 0.7925 - val_loss: 0.8843 - val_acc: 0.7820\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8469 - acc: 0.7907 - val_loss: 0.8826 - val_acc: 0.7820\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8440 - acc: 0.7921 - val_loss: 0.9187 - val_acc: 0.7500\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8468 - acc: 0.7901 - val_loss: 0.8854 - val_acc: 0.7810\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8462 - acc: 0.7939 - val_loss: 0.8925 - val_acc: 0.7750\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8450 - acc: 0.7909 - val_loss: 0.9043 - val_acc: 0.7780\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8468 - acc: 0.7936 - val_loss: 0.8837 - val_acc: 0.7840\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8453 - acc: 0.7931 - val_loss: 0.8846 - val_acc: 0.7840\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8448 - acc: 0.7913 - val_loss: 0.8825 - val_acc: 0.7820\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8457 - acc: 0.7904 - val_loss: 0.8873 - val_acc: 0.7860\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8462 - acc: 0.7919 - val_loss: 0.9013 - val_acc: 0.7790\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8446 - acc: 0.7892 - val_loss: 0.9126 - val_acc: 0.7640\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8454 - acc: 0.7937 - val_loss: 0.8897 - val_acc: 0.7850\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8444 - acc: 0.7928 - val_loss: 0.8855 - val_acc: 0.7840\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8440 - acc: 0.7911 - val_loss: 0.8895 - val_acc: 0.7770\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8438 - acc: 0.7924 - val_loss: 0.8973 - val_acc: 0.7810\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8455 - acc: 0.7903 - val_loss: 0.8833 - val_acc: 0.7870\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8434 - acc: 0.7923 - val_loss: 0.8846 - val_acc: 0.7780\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8431 - acc: 0.7921 - val_loss: 0.8837 - val_acc: 0.7870\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8423 - acc: 0.7963 - val_loss: 0.8869 - val_acc: 0.7830\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8432 - acc: 0.7928 - val_loss: 0.8832 - val_acc: 0.7810\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8428 - acc: 0.7904 - val_loss: 0.8947 - val_acc: 0.7830\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8440 - acc: 0.7947 - val_loss: 0.8818 - val_acc: 0.7790\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8420 - acc: 0.7917 - val_loss: 0.8940 - val_acc: 0.7700\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8430 - acc: 0.7908 - val_loss: 0.8847 - val_acc: 0.7800\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8435 - acc: 0.7927 - val_loss: 0.8865 - val_acc: 0.7790\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8426 - acc: 0.7943 - val_loss: 0.8859 - val_acc: 0.7760\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8425 - acc: 0.7939 - val_loss: 0.9050 - val_acc: 0.7740\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8425 - acc: 0.7928 - val_loss: 0.8997 - val_acc: 0.7780\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8421 - acc: 0.7923 - val_loss: 0.8893 - val_acc: 0.7820\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8438 - acc: 0.7913 - val_loss: 0.8822 - val_acc: 0.7750\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8423 - acc: 0.7928 - val_loss: 0.9366 - val_acc: 0.7670\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8441 - acc: 0.7939 - val_loss: 0.9119 - val_acc: 0.7780\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8423 - acc: 0.7931 - val_loss: 0.8834 - val_acc: 0.7760\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8413 - acc: 0.7931 - val_loss: 0.9075 - val_acc: 0.7730\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8414 - acc: 0.7947 - val_loss: 0.8813 - val_acc: 0.7880\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8412 - acc: 0.7937 - val_loss: 0.8936 - val_acc: 0.7750\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8421 - acc: 0.7945 - val_loss: 0.9002 - val_acc: 0.7770\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8423 - acc: 0.7956 - val_loss: 0.8854 - val_acc: 0.7820\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8402 - acc: 0.7943 - val_loss: 0.8942 - val_acc: 0.7740\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8407 - acc: 0.7924 - val_loss: 0.8893 - val_acc: 0.7760\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8400 - acc: 0.7920 - val_loss: 0.8843 - val_acc: 0.7790\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8406 - acc: 0.7933 - val_loss: 0.8810 - val_acc: 0.7850\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8395 - acc: 0.7919 - val_loss: 0.8845 - val_acc: 0.7820\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8421 - acc: 0.7964 - val_loss: 0.8804 - val_acc: 0.7780\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8397 - acc: 0.7951 - val_loss: 0.8851 - val_acc: 0.7800\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8389 - acc: 0.7944 - val_loss: 0.8923 - val_acc: 0.7720\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8410 - acc: 0.7916 - val_loss: 0.8793 - val_acc: 0.7850\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8389 - acc: 0.7963 - val_loss: 0.8828 - val_acc: 0.7770\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8384 - acc: 0.7959 - val_loss: 0.8856 - val_acc: 0.7740\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8400 - acc: 0.7955 - val_loss: 0.8850 - val_acc: 0.7830\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8390 - acc: 0.7948 - val_loss: 0.8872 - val_acc: 0.7780\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8390 - acc: 0.7921 - val_loss: 0.8819 - val_acc: 0.7850\n",
      "Epoch 767/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8388 - acc: 0.7952 - val_loss: 0.8823 - val_acc: 0.7810\n",
      "Epoch 768/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8390 - acc: 0.7949 - val_loss: 0.8953 - val_acc: 0.7810\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8413 - acc: 0.7935 - val_loss: 0.8839 - val_acc: 0.7810\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8383 - acc: 0.7947 - val_loss: 0.8879 - val_acc: 0.7760\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8401 - acc: 0.7968 - val_loss: 0.8827 - val_acc: 0.7860\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8377 - acc: 0.7955 - val_loss: 0.9031 - val_acc: 0.7790\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8385 - acc: 0.7961 - val_loss: 0.8869 - val_acc: 0.7760\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8389 - acc: 0.7945 - val_loss: 0.8810 - val_acc: 0.7860\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8374 - acc: 0.7943 - val_loss: 0.8809 - val_acc: 0.7860\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8391 - acc: 0.7928 - val_loss: 0.8962 - val_acc: 0.7690\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8400 - acc: 0.7924 - val_loss: 0.8813 - val_acc: 0.7780\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8376 - acc: 0.7963 - val_loss: 0.8876 - val_acc: 0.7710\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8381 - acc: 0.7943 - val_loss: 0.8806 - val_acc: 0.7850\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8374 - acc: 0.7933 - val_loss: 0.8887 - val_acc: 0.7730\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8361 - acc: 0.7945 - val_loss: 0.8795 - val_acc: 0.7790\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8378 - acc: 0.7963 - val_loss: 0.8880 - val_acc: 0.7850\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8382 - acc: 0.7928 - val_loss: 0.8798 - val_acc: 0.7820\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8359 - acc: 0.7971 - val_loss: 0.8801 - val_acc: 0.7870\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8384 - acc: 0.7949 - val_loss: 0.8794 - val_acc: 0.7860\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8377 - acc: 0.7963 - val_loss: 0.8877 - val_acc: 0.7810\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8371 - acc: 0.7961 - val_loss: 0.9023 - val_acc: 0.7740\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8373 - acc: 0.7989 - val_loss: 0.9211 - val_acc: 0.7680\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8367 - acc: 0.7963 - val_loss: 0.8993 - val_acc: 0.7760\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8365 - acc: 0.7960 - val_loss: 0.8887 - val_acc: 0.7780\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8355 - acc: 0.7969 - val_loss: 0.9002 - val_acc: 0.7700\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8358 - acc: 0.7968 - val_loss: 0.8993 - val_acc: 0.7830\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8390 - acc: 0.7968 - val_loss: 0.8859 - val_acc: 0.7800\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8361 - acc: 0.7968 - val_loss: 0.8826 - val_acc: 0.7830\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8360 - acc: 0.7947 - val_loss: 0.8808 - val_acc: 0.7820\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8354 - acc: 0.7973 - val_loss: 0.8828 - val_acc: 0.7790\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8338 - acc: 0.7955 - val_loss: 0.8913 - val_acc: 0.7730\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8367 - acc: 0.7957 - val_loss: 0.8972 - val_acc: 0.7840\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8348 - acc: 0.7965 - val_loss: 0.8832 - val_acc: 0.7790\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8356 - acc: 0.7940 - val_loss: 0.8844 - val_acc: 0.7820\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8358 - acc: 0.7975 - val_loss: 0.8779 - val_acc: 0.7870\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8354 - acc: 0.7961 - val_loss: 0.8796 - val_acc: 0.7840\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8340 - acc: 0.7955 - val_loss: 0.8781 - val_acc: 0.7870\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8332 - acc: 0.7965 - val_loss: 0.8893 - val_acc: 0.7790\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8341 - acc: 0.7987 - val_loss: 0.8833 - val_acc: 0.7860\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8342 - acc: 0.7969 - val_loss: 0.9129 - val_acc: 0.7770\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8377 - acc: 0.7925 - val_loss: 0.8776 - val_acc: 0.7890\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8330 - acc: 0.7973 - val_loss: 0.8793 - val_acc: 0.7800\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8327 - acc: 0.7969 - val_loss: 0.9022 - val_acc: 0.7710\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8336 - acc: 0.7952 - val_loss: 0.9033 - val_acc: 0.7700\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8370 - acc: 0.7933 - val_loss: 0.8888 - val_acc: 0.7740\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8329 - acc: 0.7985 - val_loss: 0.9064 - val_acc: 0.7740\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8370 - acc: 0.7957 - val_loss: 0.8871 - val_acc: 0.7720\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8330 - acc: 0.7972 - val_loss: 0.8872 - val_acc: 0.7750\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8338 - acc: 0.7961 - val_loss: 0.8819 - val_acc: 0.7810\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8328 - acc: 0.7971 - val_loss: 0.8826 - val_acc: 0.7790\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8327 - acc: 0.7955 - val_loss: 0.8945 - val_acc: 0.7820\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8318 - acc: 0.7985 - val_loss: 0.8809 - val_acc: 0.7790\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8322 - acc: 0.7980 - val_loss: 0.8957 - val_acc: 0.7800\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8322 - acc: 0.7973 - val_loss: 0.8853 - val_acc: 0.7840\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8330 - acc: 0.7975 - val_loss: 0.8849 - val_acc: 0.7800\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8321 - acc: 0.7979 - val_loss: 0.8959 - val_acc: 0.7820\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8328 - acc: 0.7973 - val_loss: 0.8815 - val_acc: 0.7850\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8325 - acc: 0.7976 - val_loss: 0.8794 - val_acc: 0.7820\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8325 - acc: 0.7960 - val_loss: 0.8800 - val_acc: 0.7840\n",
      "Epoch 826/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8309 - acc: 0.7965 - val_loss: 0.8869 - val_acc: 0.7770\n",
      "Epoch 827/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8311 - acc: 0.7975 - val_loss: 0.8780 - val_acc: 0.7900\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8312 - acc: 0.7968 - val_loss: 0.8917 - val_acc: 0.7790\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8336 - acc: 0.7971 - val_loss: 0.8899 - val_acc: 0.7750\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8315 - acc: 0.7984 - val_loss: 0.8856 - val_acc: 0.7810\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8316 - acc: 0.7988 - val_loss: 0.8832 - val_acc: 0.7870\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8332 - acc: 0.7971 - val_loss: 0.8888 - val_acc: 0.7800\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8319 - acc: 0.7995 - val_loss: 0.8774 - val_acc: 0.7830\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8314 - acc: 0.7972 - val_loss: 0.8870 - val_acc: 0.7850\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8301 - acc: 0.7972 - val_loss: 0.8839 - val_acc: 0.7860\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8315 - acc: 0.7985 - val_loss: 0.8801 - val_acc: 0.7880\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8331 - acc: 0.7947 - val_loss: 0.8952 - val_acc: 0.7830\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8300 - acc: 0.7993 - val_loss: 0.8967 - val_acc: 0.7780\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8316 - acc: 0.7965 - val_loss: 0.8776 - val_acc: 0.7850\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8301 - acc: 0.7981 - val_loss: 0.8813 - val_acc: 0.7850\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8294 - acc: 0.8005 - val_loss: 0.8992 - val_acc: 0.7820\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8305 - acc: 0.7973 - val_loss: 0.8770 - val_acc: 0.7910\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8291 - acc: 0.7995 - val_loss: 0.8826 - val_acc: 0.7820\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8294 - acc: 0.7984 - val_loss: 0.9032 - val_acc: 0.7800\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8296 - acc: 0.7995 - val_loss: 0.8820 - val_acc: 0.7840\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8318 - acc: 0.7979 - val_loss: 0.9109 - val_acc: 0.7820\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8314 - acc: 0.7977 - val_loss: 0.8815 - val_acc: 0.7840\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8281 - acc: 0.7999 - val_loss: 0.8924 - val_acc: 0.7820\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8301 - acc: 0.7989 - val_loss: 0.8900 - val_acc: 0.7780\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8301 - acc: 0.7983 - val_loss: 0.8802 - val_acc: 0.7830\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8285 - acc: 0.8011 - val_loss: 0.8919 - val_acc: 0.7730\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8287 - acc: 0.8005 - val_loss: 0.8824 - val_acc: 0.7840\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8266 - acc: 0.8008 - val_loss: 0.8833 - val_acc: 0.7830\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8285 - acc: 0.8000 - val_loss: 0.8771 - val_acc: 0.7860\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8287 - acc: 0.7988 - val_loss: 0.8921 - val_acc: 0.7860\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8283 - acc: 0.7971 - val_loss: 0.8828 - val_acc: 0.7810\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8270 - acc: 0.8001 - val_loss: 0.8812 - val_acc: 0.7820\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8290 - acc: 0.7981 - val_loss: 0.8802 - val_acc: 0.7840\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8282 - acc: 0.8021 - val_loss: 0.8976 - val_acc: 0.7680\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8279 - acc: 0.7984 - val_loss: 0.8945 - val_acc: 0.7700\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8277 - acc: 0.8024 - val_loss: 0.8868 - val_acc: 0.7860\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8270 - acc: 0.7979 - val_loss: 0.8819 - val_acc: 0.7790\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8291 - acc: 0.7988 - val_loss: 0.8818 - val_acc: 0.7840\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8265 - acc: 0.7987 - val_loss: 0.8880 - val_acc: 0.7800\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8272 - acc: 0.7993 - val_loss: 0.8792 - val_acc: 0.7870\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8273 - acc: 0.8013 - val_loss: 0.8761 - val_acc: 0.7860\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8270 - acc: 0.7983 - val_loss: 0.8841 - val_acc: 0.7820\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8270 - acc: 0.7987 - val_loss: 0.8770 - val_acc: 0.7780\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8271 - acc: 0.8015 - val_loss: 0.8813 - val_acc: 0.7810\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8273 - acc: 0.7976 - val_loss: 0.8883 - val_acc: 0.7780\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8247 - acc: 0.7993 - val_loss: 0.9146 - val_acc: 0.7780\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8283 - acc: 0.7989 - val_loss: 0.8813 - val_acc: 0.7830\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8260 - acc: 0.8023 - val_loss: 0.8891 - val_acc: 0.7900\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8279 - acc: 0.8012 - val_loss: 0.8811 - val_acc: 0.7830\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8266 - acc: 0.7995 - val_loss: 0.8790 - val_acc: 0.7850\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8266 - acc: 0.8027 - val_loss: 0.8876 - val_acc: 0.7820\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8281 - acc: 0.8024 - val_loss: 0.8894 - val_acc: 0.7750\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8250 - acc: 0.8007 - val_loss: 0.8775 - val_acc: 0.7800\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8258 - acc: 0.8031 - val_loss: 0.8886 - val_acc: 0.7810\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8270 - acc: 0.7996 - val_loss: 0.8804 - val_acc: 0.7800\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8263 - acc: 0.8007 - val_loss: 0.8799 - val_acc: 0.7890\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8256 - acc: 0.8004 - val_loss: 0.8852 - val_acc: 0.7940\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8267 - acc: 0.7971 - val_loss: 0.8929 - val_acc: 0.7840\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8250 - acc: 0.8017 - val_loss: 0.8922 - val_acc: 0.7810\n",
      "Epoch 885/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8249 - acc: 0.8016 - val_loss: 0.8954 - val_acc: 0.7770\n",
      "Epoch 886/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8270 - acc: 0.8032 - val_loss: 0.8948 - val_acc: 0.7800\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8241 - acc: 0.8003 - val_loss: 0.8827 - val_acc: 0.7850\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8252 - acc: 0.8027 - val_loss: 0.8885 - val_acc: 0.7860\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8256 - acc: 0.8033 - val_loss: 0.8966 - val_acc: 0.7700\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8255 - acc: 0.8015 - val_loss: 0.8834 - val_acc: 0.7830\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8241 - acc: 0.8009 - val_loss: 0.9061 - val_acc: 0.7680\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8245 - acc: 0.8019 - val_loss: 0.8854 - val_acc: 0.7840\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8223 - acc: 0.8021 - val_loss: 0.8868 - val_acc: 0.7810\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8246 - acc: 0.8011 - val_loss: 0.8796 - val_acc: 0.7860\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8242 - acc: 0.8025 - val_loss: 0.8756 - val_acc: 0.7910\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8235 - acc: 0.8025 - val_loss: 0.8860 - val_acc: 0.7790\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8241 - acc: 0.8015 - val_loss: 0.8919 - val_acc: 0.7830\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8257 - acc: 0.8011 - val_loss: 0.8818 - val_acc: 0.7900\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8263 - acc: 0.8023 - val_loss: 0.8757 - val_acc: 0.7920\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8228 - acc: 0.7989 - val_loss: 0.9042 - val_acc: 0.7810\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8250 - acc: 0.8020 - val_loss: 0.8756 - val_acc: 0.7950\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8237 - acc: 0.8035 - val_loss: 0.8882 - val_acc: 0.7840\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8218 - acc: 0.8016 - val_loss: 0.8838 - val_acc: 0.7850\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8235 - acc: 0.8001 - val_loss: 0.8829 - val_acc: 0.7780\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8216 - acc: 0.8024 - val_loss: 0.8868 - val_acc: 0.7780\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8198 - acc: 0.8039 - val_loss: 0.8845 - val_acc: 0.7800\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8214 - acc: 0.8048 - val_loss: 0.8952 - val_acc: 0.7750\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8220 - acc: 0.8029 - val_loss: 0.8940 - val_acc: 0.7850\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8220 - acc: 0.8035 - val_loss: 0.9055 - val_acc: 0.7640\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8240 - acc: 0.7999 - val_loss: 0.8798 - val_acc: 0.7850\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8234 - acc: 0.8007 - val_loss: 0.8980 - val_acc: 0.7760\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8256 - acc: 0.7993 - val_loss: 0.8883 - val_acc: 0.7760\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8212 - acc: 0.8025 - val_loss: 0.8823 - val_acc: 0.7860\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8220 - acc: 0.8007 - val_loss: 0.8785 - val_acc: 0.7850\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8216 - acc: 0.8029 - val_loss: 0.8775 - val_acc: 0.7880\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8213 - acc: 0.8036 - val_loss: 0.8810 - val_acc: 0.7870\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8225 - acc: 0.8039 - val_loss: 0.9121 - val_acc: 0.7710\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8208 - acc: 0.8021 - val_loss: 0.9108 - val_acc: 0.7790\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8215 - acc: 0.8047 - val_loss: 0.9092 - val_acc: 0.7790\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8213 - acc: 0.8040 - val_loss: 0.8793 - val_acc: 0.7940\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8224 - acc: 0.8025 - val_loss: 0.8840 - val_acc: 0.7900\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8225 - acc: 0.8033 - val_loss: 0.8801 - val_acc: 0.7770\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8221 - acc: 0.8024 - val_loss: 0.8920 - val_acc: 0.7840\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8221 - acc: 0.8004 - val_loss: 0.9032 - val_acc: 0.7760\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8215 - acc: 0.8063 - val_loss: 0.8782 - val_acc: 0.7880\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8189 - acc: 0.8052 - val_loss: 0.8863 - val_acc: 0.7830\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8199 - acc: 0.8031 - val_loss: 0.8846 - val_acc: 0.7790\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8210 - acc: 0.8027 - val_loss: 0.8843 - val_acc: 0.7820\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8204 - acc: 0.8021 - val_loss: 0.8820 - val_acc: 0.7720\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8198 - acc: 0.8023 - val_loss: 0.8822 - val_acc: 0.7860\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 13us/step - loss: 0.8212 - acc: 0.8045 - val_loss: 0.8767 - val_acc: 0.7910\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8211 - acc: 0.8029 - val_loss: 0.8864 - val_acc: 0.7850\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8187 - acc: 0.8028 - val_loss: 0.8877 - val_acc: 0.7790\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8205 - acc: 0.8035 - val_loss: 0.8872 - val_acc: 0.7830\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8208 - acc: 0.8023 - val_loss: 0.8827 - val_acc: 0.7870\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8204 - acc: 0.8023 - val_loss: 0.8780 - val_acc: 0.7870\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8180 - acc: 0.8036 - val_loss: 0.8819 - val_acc: 0.7860\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8194 - acc: 0.8029 - val_loss: 0.8828 - val_acc: 0.7930\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8196 - acc: 0.8051 - val_loss: 0.8804 - val_acc: 0.7910\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8191 - acc: 0.8027 - val_loss: 0.8966 - val_acc: 0.7820\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8181 - acc: 0.8028 - val_loss: 0.8790 - val_acc: 0.7890\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8181 - acc: 0.8044 - val_loss: 0.8995 - val_acc: 0.7780\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8198 - acc: 0.8053 - val_loss: 0.8916 - val_acc: 0.7750\n",
      "Epoch 944/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8216 - acc: 0.8029 - val_loss: 0.9019 - val_acc: 0.7750\n",
      "Epoch 945/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8176 - acc: 0.8063 - val_loss: 0.8900 - val_acc: 0.7780\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8205 - acc: 0.8029 - val_loss: 0.8798 - val_acc: 0.7850\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8194 - acc: 0.8033 - val_loss: 0.8959 - val_acc: 0.7820\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8210 - acc: 0.8028 - val_loss: 0.8803 - val_acc: 0.7910\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8179 - acc: 0.8073 - val_loss: 0.8862 - val_acc: 0.7790\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8165 - acc: 0.8040 - val_loss: 0.9317 - val_acc: 0.7620\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8179 - acc: 0.8071 - val_loss: 0.8750 - val_acc: 0.7900\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8178 - acc: 0.8041 - val_loss: 0.8797 - val_acc: 0.7790\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8175 - acc: 0.8039 - val_loss: 0.8930 - val_acc: 0.7880\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8179 - acc: 0.8044 - val_loss: 0.8786 - val_acc: 0.7880\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8186 - acc: 0.8033 - val_loss: 0.8794 - val_acc: 0.7860\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8162 - acc: 0.8041 - val_loss: 0.8800 - val_acc: 0.7900\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8177 - acc: 0.8059 - val_loss: 0.8820 - val_acc: 0.7810\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8174 - acc: 0.8057 - val_loss: 0.8742 - val_acc: 0.7820\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8148 - acc: 0.8081 - val_loss: 0.8805 - val_acc: 0.7840\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8157 - acc: 0.8045 - val_loss: 0.8899 - val_acc: 0.7840\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8162 - acc: 0.8041 - val_loss: 0.9292 - val_acc: 0.7670\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8172 - acc: 0.8088 - val_loss: 0.8766 - val_acc: 0.7910\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8185 - acc: 0.8044 - val_loss: 0.8776 - val_acc: 0.7810\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8173 - acc: 0.8036 - val_loss: 0.8911 - val_acc: 0.7790\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8162 - acc: 0.8063 - val_loss: 0.8854 - val_acc: 0.7770\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8160 - acc: 0.8079 - val_loss: 0.8840 - val_acc: 0.7840\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8165 - acc: 0.8068 - val_loss: 0.8796 - val_acc: 0.7780\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8162 - acc: 0.8056 - val_loss: 0.8888 - val_acc: 0.7810\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8162 - acc: 0.8045 - val_loss: 0.8868 - val_acc: 0.7770\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8145 - acc: 0.8065 - val_loss: 0.9019 - val_acc: 0.7830\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8165 - acc: 0.8068 - val_loss: 0.8838 - val_acc: 0.7810\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8151 - acc: 0.8056 - val_loss: 0.8833 - val_acc: 0.7750\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8163 - acc: 0.8060 - val_loss: 0.9443 - val_acc: 0.7800\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8187 - acc: 0.8027 - val_loss: 0.8825 - val_acc: 0.7830\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8158 - acc: 0.8065 - val_loss: 0.8768 - val_acc: 0.7850\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8150 - acc: 0.8071 - val_loss: 0.8841 - val_acc: 0.7800\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8141 - acc: 0.8073 - val_loss: 0.8789 - val_acc: 0.7860\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8163 - acc: 0.8053 - val_loss: 0.8818 - val_acc: 0.7770\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8145 - acc: 0.8072 - val_loss: 0.8860 - val_acc: 0.7800\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8142 - acc: 0.8072 - val_loss: 0.8945 - val_acc: 0.7750\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8154 - acc: 0.8060 - val_loss: 0.9381 - val_acc: 0.7660\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8157 - acc: 0.8048 - val_loss: 0.8944 - val_acc: 0.7860\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8148 - acc: 0.8075 - val_loss: 0.8859 - val_acc: 0.7880\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8132 - acc: 0.8079 - val_loss: 0.8953 - val_acc: 0.7700\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8169 - acc: 0.8063 - val_loss: 0.9216 - val_acc: 0.7710\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8152 - acc: 0.8048 - val_loss: 0.8769 - val_acc: 0.7830\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8145 - acc: 0.8077 - val_loss: 0.9095 - val_acc: 0.7800\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8174 - acc: 0.8057 - val_loss: 0.8812 - val_acc: 0.7780\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8155 - acc: 0.8028 - val_loss: 0.8764 - val_acc: 0.7950\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8147 - acc: 0.8079 - val_loss: 0.8881 - val_acc: 0.7860\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8122 - acc: 0.8085 - val_loss: 0.9002 - val_acc: 0.7820\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 0s 14us/step - loss: 0.8130 - acc: 0.8093 - val_loss: 0.8827 - val_acc: 0.7750\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8133 - acc: 0.8056 - val_loss: 0.8779 - val_acc: 0.7820\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8132 - acc: 0.8088 - val_loss: 0.8861 - val_acc: 0.7960\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8128 - acc: 0.8065 - val_loss: 0.8856 - val_acc: 0.7740\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8132 - acc: 0.8076 - val_loss: 0.8790 - val_acc: 0.7950\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8145 - acc: 0.8069 - val_loss: 0.8991 - val_acc: 0.7670\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8133 - acc: 0.8064 - val_loss: 0.8902 - val_acc: 0.7770\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8137 - acc: 0.8075 - val_loss: 0.8871 - val_acc: 0.7720\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 15us/step - loss: 0.8137 - acc: 0.8087 - val_loss: 0.8996 - val_acc: 0.7790\n"
     ]
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 22us/step\n",
      "1500/1500 [==============================] - 0s 22us/step\n"
     ]
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8224783395131429, 0.7962666666348776]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9036528784434, 0.7673333330154419]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best result you've achieved so far, but you were training for quite a while! Next, experiment with dropout regularization to see if it offers any advantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.9610 - acc: 0.1509 - val_loss: 1.9359 - val_acc: 0.1570\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.9396 - acc: 0.1693 - val_loss: 1.9257 - val_acc: 0.1680\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9336 - acc: 0.1725 - val_loss: 1.9176 - val_acc: 0.1810\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9287 - acc: 0.1849 - val_loss: 1.9109 - val_acc: 0.1910\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9187 - acc: 0.1817 - val_loss: 1.9039 - val_acc: 0.1920\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.9145 - acc: 0.1867 - val_loss: 1.8971 - val_acc: 0.1900\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.9088 - acc: 0.1941 - val_loss: 1.8896 - val_acc: 0.1860\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.9021 - acc: 0.2044 - val_loss: 1.8808 - val_acc: 0.1930\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.8865 - acc: 0.2076 - val_loss: 1.8709 - val_acc: 0.1920\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8854 - acc: 0.2111 - val_loss: 1.8612 - val_acc: 0.1950\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8810 - acc: 0.2143 - val_loss: 1.8508 - val_acc: 0.1970\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8695 - acc: 0.2223 - val_loss: 1.8392 - val_acc: 0.2040\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8588 - acc: 0.2307 - val_loss: 1.8262 - val_acc: 0.2150\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8447 - acc: 0.2389 - val_loss: 1.8120 - val_acc: 0.2190\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8339 - acc: 0.2493 - val_loss: 1.7977 - val_acc: 0.2390\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8225 - acc: 0.2527 - val_loss: 1.7822 - val_acc: 0.2620\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8193 - acc: 0.2547 - val_loss: 1.7671 - val_acc: 0.2770\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8031 - acc: 0.2656 - val_loss: 1.7503 - val_acc: 0.2940\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7927 - acc: 0.2744 - val_loss: 1.7320 - val_acc: 0.3120\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7814 - acc: 0.2839 - val_loss: 1.7130 - val_acc: 0.3390\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7593 - acc: 0.2959 - val_loss: 1.6926 - val_acc: 0.3600\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7517 - acc: 0.3031 - val_loss: 1.6722 - val_acc: 0.3720\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7395 - acc: 0.3063 - val_loss: 1.6523 - val_acc: 0.3770\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7142 - acc: 0.3168 - val_loss: 1.6308 - val_acc: 0.3990\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6996 - acc: 0.3361 - val_loss: 1.6081 - val_acc: 0.4120\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6929 - acc: 0.3347 - val_loss: 1.5861 - val_acc: 0.4350\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6649 - acc: 0.3432 - val_loss: 1.5629 - val_acc: 0.4570\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6606 - acc: 0.3471 - val_loss: 1.5405 - val_acc: 0.4670\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6454 - acc: 0.3529 - val_loss: 1.5193 - val_acc: 0.4830\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.6219 - acc: 0.3712 - val_loss: 1.4961 - val_acc: 0.4810\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5969 - acc: 0.3796 - val_loss: 1.4729 - val_acc: 0.4970\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5867 - acc: 0.3851 - val_loss: 1.4520 - val_acc: 0.5090\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5802 - acc: 0.3915 - val_loss: 1.4315 - val_acc: 0.5220\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5605 - acc: 0.4008 - val_loss: 1.4094 - val_acc: 0.5230\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5511 - acc: 0.3927 - val_loss: 1.3909 - val_acc: 0.5370\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5306 - acc: 0.4127 - val_loss: 1.3703 - val_acc: 0.5470\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5265 - acc: 0.4139 - val_loss: 1.3522 - val_acc: 0.5550\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5032 - acc: 0.4197 - val_loss: 1.3319 - val_acc: 0.5630\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4957 - acc: 0.4205 - val_loss: 1.3130 - val_acc: 0.5650\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4774 - acc: 0.4296 - val_loss: 1.2927 - val_acc: 0.5800\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4600 - acc: 0.4392 - val_loss: 1.2755 - val_acc: 0.5800\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4551 - acc: 0.4367 - val_loss: 1.2587 - val_acc: 0.5910\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4378 - acc: 0.4467 - val_loss: 1.2407 - val_acc: 0.6040\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4254 - acc: 0.4465 - val_loss: 1.2243 - val_acc: 0.6050\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4109 - acc: 0.4600 - val_loss: 1.2067 - val_acc: 0.6060\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3954 - acc: 0.4604 - val_loss: 1.1919 - val_acc: 0.6110\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3970 - acc: 0.4664 - val_loss: 1.1781 - val_acc: 0.6230\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3806 - acc: 0.4771 - val_loss: 1.1628 - val_acc: 0.6260\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3716 - acc: 0.4717 - val_loss: 1.1477 - val_acc: 0.6260\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3580 - acc: 0.4832 - val_loss: 1.1337 - val_acc: 0.6350\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3441 - acc: 0.4920 - val_loss: 1.1199 - val_acc: 0.6390\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3335 - acc: 0.4957 - val_loss: 1.1045 - val_acc: 0.6490\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3123 - acc: 0.5005 - val_loss: 1.0902 - val_acc: 0.6500\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3165 - acc: 0.4967 - val_loss: 1.0796 - val_acc: 0.6530\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3035 - acc: 0.5013 - val_loss: 1.0662 - val_acc: 0.6650\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2895 - acc: 0.4993 - val_loss: 1.0527 - val_acc: 0.6690\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2844 - acc: 0.5068 - val_loss: 1.0410 - val_acc: 0.6730\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2781 - acc: 0.5105 - val_loss: 1.0323 - val_acc: 0.6760\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2558 - acc: 0.5188 - val_loss: 1.0170 - val_acc: 0.6800\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2356 - acc: 0.5264 - val_loss: 1.0025 - val_acc: 0.6830\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2457 - acc: 0.5271 - val_loss: 0.9947 - val_acc: 0.6820\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2294 - acc: 0.5359 - val_loss: 0.9866 - val_acc: 0.6870\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2345 - acc: 0.5317 - val_loss: 0.9769 - val_acc: 0.6940\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2212 - acc: 0.5404 - val_loss: 0.9678 - val_acc: 0.6930\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2089 - acc: 0.5463 - val_loss: 0.9577 - val_acc: 0.6990\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2080 - acc: 0.5413 - val_loss: 0.9482 - val_acc: 0.7010\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1889 - acc: 0.5528 - val_loss: 0.9365 - val_acc: 0.7010\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1824 - acc: 0.5563 - val_loss: 0.9301 - val_acc: 0.7060\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1884 - acc: 0.5532 - val_loss: 0.9211 - val_acc: 0.7160\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1560 - acc: 0.5644 - val_loss: 0.9130 - val_acc: 0.7100\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1620 - acc: 0.5592 - val_loss: 0.9055 - val_acc: 0.7090\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1670 - acc: 0.5577 - val_loss: 0.8989 - val_acc: 0.7170\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1403 - acc: 0.5587 - val_loss: 0.8899 - val_acc: 0.7150\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1426 - acc: 0.5699 - val_loss: 0.8827 - val_acc: 0.7160\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1317 - acc: 0.5656 - val_loss: 0.8754 - val_acc: 0.7200\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1224 - acc: 0.5832 - val_loss: 0.8677 - val_acc: 0.7230\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1198 - acc: 0.5780 - val_loss: 0.8620 - val_acc: 0.7230\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1135 - acc: 0.5764 - val_loss: 0.8546 - val_acc: 0.7260\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1095 - acc: 0.5825 - val_loss: 0.8499 - val_acc: 0.7270\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1155 - acc: 0.5756 - val_loss: 0.8452 - val_acc: 0.7200\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0909 - acc: 0.5839 - val_loss: 0.8383 - val_acc: 0.7290\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0916 - acc: 0.5887 - val_loss: 0.8317 - val_acc: 0.7300\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0772 - acc: 0.5943 - val_loss: 0.8243 - val_acc: 0.7280\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0717 - acc: 0.5925 - val_loss: 0.8199 - val_acc: 0.7350\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0726 - acc: 0.5956 - val_loss: 0.8128 - val_acc: 0.7370\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0700 - acc: 0.6005 - val_loss: 0.8095 - val_acc: 0.7390\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0636 - acc: 0.5981 - val_loss: 0.8034 - val_acc: 0.7370\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0530 - acc: 0.6021 - val_loss: 0.7973 - val_acc: 0.7350\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0480 - acc: 0.6063 - val_loss: 0.7919 - val_acc: 0.7380\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0559 - acc: 0.6041 - val_loss: 0.7897 - val_acc: 0.7400\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0451 - acc: 0.6068 - val_loss: 0.7854 - val_acc: 0.7440\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0395 - acc: 0.6059 - val_loss: 0.7819 - val_acc: 0.7440\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0356 - acc: 0.6121 - val_loss: 0.7768 - val_acc: 0.7380\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0296 - acc: 0.6225 - val_loss: 0.7719 - val_acc: 0.7400\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0206 - acc: 0.6171 - val_loss: 0.7664 - val_acc: 0.7410\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0225 - acc: 0.6204 - val_loss: 0.7641 - val_acc: 0.7450\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0141 - acc: 0.6251 - val_loss: 0.7618 - val_acc: 0.7420\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0079 - acc: 0.6232 - val_loss: 0.7583 - val_acc: 0.7430\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0132 - acc: 0.6229 - val_loss: 0.7548 - val_acc: 0.7430\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9964 - acc: 0.6163 - val_loss: 0.7488 - val_acc: 0.7450\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9813 - acc: 0.6339 - val_loss: 0.7433 - val_acc: 0.7420\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9743 - acc: 0.6317 - val_loss: 0.7390 - val_acc: 0.7460\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9870 - acc: 0.6327 - val_loss: 0.7366 - val_acc: 0.7450\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9703 - acc: 0.6345 - val_loss: 0.7324 - val_acc: 0.7480\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9700 - acc: 0.6388 - val_loss: 0.7320 - val_acc: 0.7470\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9727 - acc: 0.6329 - val_loss: 0.7256 - val_acc: 0.7540\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9710 - acc: 0.6343 - val_loss: 0.7243 - val_acc: 0.7510\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9628 - acc: 0.6392 - val_loss: 0.7204 - val_acc: 0.7520\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9580 - acc: 0.6451 - val_loss: 0.7170 - val_acc: 0.7530\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9702 - acc: 0.6317 - val_loss: 0.7160 - val_acc: 0.7540\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9601 - acc: 0.6429 - val_loss: 0.7129 - val_acc: 0.7500\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9536 - acc: 0.6401 - val_loss: 0.7083 - val_acc: 0.7570\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9368 - acc: 0.6439 - val_loss: 0.7040 - val_acc: 0.7540\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9465 - acc: 0.6480 - val_loss: 0.7026 - val_acc: 0.7570\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9605 - acc: 0.6433 - val_loss: 0.7028 - val_acc: 0.7560\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9476 - acc: 0.6379 - val_loss: 0.6993 - val_acc: 0.7600\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9416 - acc: 0.6448 - val_loss: 0.6982 - val_acc: 0.7560\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9288 - acc: 0.6488 - val_loss: 0.6943 - val_acc: 0.7570\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9342 - acc: 0.6436 - val_loss: 0.6922 - val_acc: 0.7620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9258 - acc: 0.6537 - val_loss: 0.6903 - val_acc: 0.7540\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9308 - acc: 0.6511 - val_loss: 0.6881 - val_acc: 0.7570\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9253 - acc: 0.6573 - val_loss: 0.6865 - val_acc: 0.7570\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9111 - acc: 0.6545 - val_loss: 0.6842 - val_acc: 0.7580\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9187 - acc: 0.6604 - val_loss: 0.6829 - val_acc: 0.7570\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9143 - acc: 0.6593 - val_loss: 0.6781 - val_acc: 0.7610\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9084 - acc: 0.6652 - val_loss: 0.6756 - val_acc: 0.7620\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8999 - acc: 0.6572 - val_loss: 0.6748 - val_acc: 0.7610\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9022 - acc: 0.6643 - val_loss: 0.6726 - val_acc: 0.7620\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8974 - acc: 0.6639 - val_loss: 0.6725 - val_acc: 0.7610\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8816 - acc: 0.6672 - val_loss: 0.6687 - val_acc: 0.7620\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8949 - acc: 0.6667 - val_loss: 0.6665 - val_acc: 0.7650\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8908 - acc: 0.6628 - val_loss: 0.6647 - val_acc: 0.7640\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8916 - acc: 0.6684 - val_loss: 0.6637 - val_acc: 0.7620\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8732 - acc: 0.6749 - val_loss: 0.6589 - val_acc: 0.7630\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8817 - acc: 0.6731 - val_loss: 0.6584 - val_acc: 0.7670\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8798 - acc: 0.6665 - val_loss: 0.6576 - val_acc: 0.7670\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8723 - acc: 0.6759 - val_loss: 0.6553 - val_acc: 0.7670\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8853 - acc: 0.6675 - val_loss: 0.6565 - val_acc: 0.7610\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8740 - acc: 0.6724 - val_loss: 0.6517 - val_acc: 0.7680\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8615 - acc: 0.6800 - val_loss: 0.6491 - val_acc: 0.7710\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8684 - acc: 0.6759 - val_loss: 0.6487 - val_acc: 0.7700\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8462 - acc: 0.6769 - val_loss: 0.6448 - val_acc: 0.7710\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8493 - acc: 0.6764 - val_loss: 0.6441 - val_acc: 0.7740\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8806 - acc: 0.6729 - val_loss: 0.6448 - val_acc: 0.7700\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8598 - acc: 0.6787 - val_loss: 0.6446 - val_acc: 0.7710\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8612 - acc: 0.6787 - val_loss: 0.6427 - val_acc: 0.7710\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8461 - acc: 0.6792 - val_loss: 0.6427 - val_acc: 0.7720\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8538 - acc: 0.6809 - val_loss: 0.6400 - val_acc: 0.7700\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8544 - acc: 0.6812 - val_loss: 0.6387 - val_acc: 0.7720\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8582 - acc: 0.6829 - val_loss: 0.6382 - val_acc: 0.7690\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8322 - acc: 0.6880 - val_loss: 0.6361 - val_acc: 0.7740\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8284 - acc: 0.6901 - val_loss: 0.6325 - val_acc: 0.7730\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8297 - acc: 0.6951 - val_loss: 0.6303 - val_acc: 0.7710\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8298 - acc: 0.6935 - val_loss: 0.6284 - val_acc: 0.7790\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8395 - acc: 0.6875 - val_loss: 0.6274 - val_acc: 0.7710\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8333 - acc: 0.6923 - val_loss: 0.6264 - val_acc: 0.7730\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8220 - acc: 0.6969 - val_loss: 0.6248 - val_acc: 0.7740\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8409 - acc: 0.6872 - val_loss: 0.6247 - val_acc: 0.7720\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8299 - acc: 0.6949 - val_loss: 0.6255 - val_acc: 0.7740\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8299 - acc: 0.6928 - val_loss: 0.6235 - val_acc: 0.7750\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8301 - acc: 0.6943 - val_loss: 0.6234 - val_acc: 0.7790\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8075 - acc: 0.7037 - val_loss: 0.6197 - val_acc: 0.7760\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8180 - acc: 0.6931 - val_loss: 0.6192 - val_acc: 0.7800\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8035 - acc: 0.6971 - val_loss: 0.6182 - val_acc: 0.7750\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8055 - acc: 0.6956 - val_loss: 0.6164 - val_acc: 0.7740\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8248 - acc: 0.6911 - val_loss: 0.6177 - val_acc: 0.7750\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8188 - acc: 0.6957 - val_loss: 0.6168 - val_acc: 0.7750\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8145 - acc: 0.6939 - val_loss: 0.6164 - val_acc: 0.7720\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8132 - acc: 0.6977 - val_loss: 0.6143 - val_acc: 0.7760\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8000 - acc: 0.7032 - val_loss: 0.6128 - val_acc: 0.7740\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7947 - acc: 0.6999 - val_loss: 0.6128 - val_acc: 0.7760\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7933 - acc: 0.6993 - val_loss: 0.6106 - val_acc: 0.7750\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7968 - acc: 0.7000 - val_loss: 0.6102 - val_acc: 0.7790\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8083 - acc: 0.6997 - val_loss: 0.6082 - val_acc: 0.7780\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7887 - acc: 0.7020 - val_loss: 0.6092 - val_acc: 0.7810\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7854 - acc: 0.7083 - val_loss: 0.6082 - val_acc: 0.7810\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7763 - acc: 0.7116 - val_loss: 0.6063 - val_acc: 0.7760\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7865 - acc: 0.7017 - val_loss: 0.6063 - val_acc: 0.7770\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7852 - acc: 0.7145 - val_loss: 0.6058 - val_acc: 0.7760\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7945 - acc: 0.6997 - val_loss: 0.6056 - val_acc: 0.7790\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7897 - acc: 0.7055 - val_loss: 0.6041 - val_acc: 0.7790\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7765 - acc: 0.7017 - val_loss: 0.6018 - val_acc: 0.7780\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7888 - acc: 0.7088 - val_loss: 0.6019 - val_acc: 0.7800\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7684 - acc: 0.7144 - val_loss: 0.6000 - val_acc: 0.7810\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7768 - acc: 0.7099 - val_loss: 0.5987 - val_acc: 0.7790\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7712 - acc: 0.7109 - val_loss: 0.5984 - val_acc: 0.7800\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7652 - acc: 0.7101 - val_loss: 0.5984 - val_acc: 0.7810\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7751 - acc: 0.7125 - val_loss: 0.5988 - val_acc: 0.7780\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7703 - acc: 0.7132 - val_loss: 0.5969 - val_acc: 0.7770\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7669 - acc: 0.7085 - val_loss: 0.5957 - val_acc: 0.7760\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7584 - acc: 0.7080 - val_loss: 0.5958 - val_acc: 0.7820\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7460 - acc: 0.7177 - val_loss: 0.5928 - val_acc: 0.7820\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7613 - acc: 0.7211 - val_loss: 0.5910 - val_acc: 0.7820\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7572 - acc: 0.7164 - val_loss: 0.5921 - val_acc: 0.7840\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7562 - acc: 0.7120 - val_loss: 0.5907 - val_acc: 0.7820\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7629 - acc: 0.7187 - val_loss: 0.5910 - val_acc: 0.7800\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7484 - acc: 0.7188 - val_loss: 0.5890 - val_acc: 0.7820\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.7539 - acc: 0.7197 - val_loss: 0.5899 - val_acc: 0.7830\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7448 - acc: 0.7137 - val_loss: 0.5892 - val_acc: 0.7850\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7518 - acc: 0.7175 - val_loss: 0.5901 - val_acc: 0.7850\n"
     ]
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 22us/step\n",
      "1500/1500 [==============================] - 0s 22us/step\n"
     ]
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4718774256070455, 0.8294666666348776]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6141259970664978, 0.7619999996821085]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! The variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. You actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple your data set, and see what happens. Note that you are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# __SOLUTION__ \n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 1.9124 - acc: 0.1945 - val_loss: 1.8530 - val_acc: 0.2520\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 1.7752 - acc: 0.3128 - val_loss: 1.6854 - val_acc: 0.3737\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 1.5886 - acc: 0.4248 - val_loss: 1.4948 - val_acc: 0.4693\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 1.3949 - acc: 0.5276 - val_loss: 1.3089 - val_acc: 0.5733\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 0s 13us/step - loss: 1.2185 - acc: 0.6142 - val_loss: 1.1536 - val_acc: 0.6440\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 1.0738 - acc: 0.6624 - val_loss: 1.0292 - val_acc: 0.6740\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.9595 - acc: 0.6946 - val_loss: 0.9336 - val_acc: 0.6857\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.8732 - acc: 0.7142 - val_loss: 0.8624 - val_acc: 0.7050\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.8087 - acc: 0.7277 - val_loss: 0.8095 - val_acc: 0.7193\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.7605 - acc: 0.7379 - val_loss: 0.7718 - val_acc: 0.7300\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 0s 13us/step - loss: 0.7237 - acc: 0.7471 - val_loss: 0.7406 - val_acc: 0.7353\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 0s 13us/step - loss: 0.6951 - acc: 0.7532 - val_loss: 0.7173 - val_acc: 0.7390\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.6715 - acc: 0.7608 - val_loss: 0.7006 - val_acc: 0.7413\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 15us/step - loss: 0.6525 - acc: 0.7653 - val_loss: 0.6858 - val_acc: 0.7510\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 0s 13us/step - loss: 0.6360 - acc: 0.7699 - val_loss: 0.6728 - val_acc: 0.7567\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.6219 - acc: 0.7745 - val_loss: 0.6616 - val_acc: 0.7543\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.6093 - acc: 0.7801 - val_loss: 0.6544 - val_acc: 0.7567\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.5978 - acc: 0.7833 - val_loss: 0.6441 - val_acc: 0.7593\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.5876 - acc: 0.7858 - val_loss: 0.6390 - val_acc: 0.7633\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.5783 - acc: 0.7900 - val_loss: 0.6321 - val_acc: 0.7663\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 0s 13us/step - loss: 0.5696 - acc: 0.7926 - val_loss: 0.6254 - val_acc: 0.7650\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.5614 - acc: 0.7961 - val_loss: 0.6197 - val_acc: 0.7667\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.5537 - acc: 0.7987 - val_loss: 0.6182 - val_acc: 0.7733\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 0s 13us/step - loss: 0.5468 - acc: 0.8009 - val_loss: 0.6116 - val_acc: 0.7750\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.5398 - acc: 0.8041 - val_loss: 0.6085 - val_acc: 0.7753\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 0s 13us/step - loss: 0.5333 - acc: 0.8068 - val_loss: 0.6040 - val_acc: 0.7773\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 0s 13us/step - loss: 0.5272 - acc: 0.8090 - val_loss: 0.6011 - val_acc: 0.7770\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.5212 - acc: 0.8095 - val_loss: 0.5966 - val_acc: 0.7813\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 0s 13us/step - loss: 0.5158 - acc: 0.8141 - val_loss: 0.5945 - val_acc: 0.7827\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.5100 - acc: 0.8160 - val_loss: 0.5934 - val_acc: 0.7837\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 0s 13us/step - loss: 0.5052 - acc: 0.8176 - val_loss: 0.5910 - val_acc: 0.7843\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.5000 - acc: 0.8202 - val_loss: 0.5868 - val_acc: 0.7890\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.4952 - acc: 0.8208 - val_loss: 0.5849 - val_acc: 0.7893\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.4906 - acc: 0.8222 - val_loss: 0.5831 - val_acc: 0.7890\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.4862 - acc: 0.8248 - val_loss: 0.5819 - val_acc: 0.7893\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.4821 - acc: 0.8269 - val_loss: 0.5785 - val_acc: 0.7907\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.4773 - acc: 0.8290 - val_loss: 0.5779 - val_acc: 0.7930\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 0s 11us/step - loss: 0.4735 - acc: 0.8299 - val_loss: 0.5753 - val_acc: 0.7957\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.4693 - acc: 0.8311 - val_loss: 0.5742 - val_acc: 0.7990\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.4657 - acc: 0.8339 - val_loss: 0.5732 - val_acc: 0.7963\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.4618 - acc: 0.8348 - val_loss: 0.5725 - val_acc: 0.7947\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 0s 11us/step - loss: 0.4586 - acc: 0.8352 - val_loss: 0.5702 - val_acc: 0.8000\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.4548 - acc: 0.8374 - val_loss: 0.5691 - val_acc: 0.7970\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 0s 13us/step - loss: 0.4513 - acc: 0.8385 - val_loss: 0.5707 - val_acc: 0.8013\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 0s 13us/step - loss: 0.4482 - acc: 0.8395 - val_loss: 0.5692 - val_acc: 0.7950\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.4447 - acc: 0.8410 - val_loss: 0.5659 - val_acc: 0.8003\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 0s 13us/step - loss: 0.4416 - acc: 0.8423 - val_loss: 0.5657 - val_acc: 0.7983\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.4389 - acc: 0.8426 - val_loss: 0.5654 - val_acc: 0.8013\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.4357 - acc: 0.8440 - val_loss: 0.5634 - val_acc: 0.8003\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 0s 15us/step - loss: 0.4328 - acc: 0.8458 - val_loss: 0.5629 - val_acc: 0.8020\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 0s 13us/step - loss: 0.4299 - acc: 0.8477 - val_loss: 0.5643 - val_acc: 0.8020\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 0s 13us/step - loss: 0.4272 - acc: 0.8483 - val_loss: 0.5629 - val_acc: 0.8030\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.4246 - acc: 0.8490 - val_loss: 0.5614 - val_acc: 0.8047\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 0s 13us/step - loss: 0.4218 - acc: 0.8500 - val_loss: 0.5611 - val_acc: 0.7993\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 0s 13us/step - loss: 0.4194 - acc: 0.8502 - val_loss: 0.5607 - val_acc: 0.8037\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.4169 - acc: 0.8510 - val_loss: 0.5616 - val_acc: 0.8037\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.4144 - acc: 0.8525 - val_loss: 0.5635 - val_acc: 0.8027\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.4118 - acc: 0.8529 - val_loss: 0.5621 - val_acc: 0.8003\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.4093 - acc: 0.8552 - val_loss: 0.5637 - val_acc: 0.8040\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 0s 13us/step - loss: 0.4070 - acc: 0.8550 - val_loss: 0.5586 - val_acc: 0.8050\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.4048 - acc: 0.8568 - val_loss: 0.5604 - val_acc: 0.8033\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.4025 - acc: 0.8572 - val_loss: 0.5601 - val_acc: 0.8007\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.4001 - acc: 0.8585 - val_loss: 0.5605 - val_acc: 0.8013\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3985 - acc: 0.8589 - val_loss: 0.5612 - val_acc: 0.8030\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3961 - acc: 0.8594 - val_loss: 0.5606 - val_acc: 0.8040\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3941 - acc: 0.8602 - val_loss: 0.5612 - val_acc: 0.8017\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 0s 11us/step - loss: 0.3922 - acc: 0.8618 - val_loss: 0.5628 - val_acc: 0.8030\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3900 - acc: 0.8621 - val_loss: 0.5608 - val_acc: 0.8037\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3877 - acc: 0.8625 - val_loss: 0.5615 - val_acc: 0.8013\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3861 - acc: 0.8635 - val_loss: 0.5600 - val_acc: 0.8040\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3840 - acc: 0.8638 - val_loss: 0.5607 - val_acc: 0.8020\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3824 - acc: 0.8654 - val_loss: 0.5610 - val_acc: 0.8037\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3803 - acc: 0.8666 - val_loss: 0.5620 - val_acc: 0.8027\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3786 - acc: 0.8664 - val_loss: 0.5627 - val_acc: 0.8040\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3768 - acc: 0.8671 - val_loss: 0.5617 - val_acc: 0.8027\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3749 - acc: 0.8672 - val_loss: 0.5609 - val_acc: 0.8027\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3734 - acc: 0.8685 - val_loss: 0.5616 - val_acc: 0.8030\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3714 - acc: 0.8692 - val_loss: 0.5626 - val_acc: 0.8020\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3698 - acc: 0.8699 - val_loss: 0.5629 - val_acc: 0.8017\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3682 - acc: 0.8705 - val_loss: 0.5638 - val_acc: 0.8033\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3665 - acc: 0.8699 - val_loss: 0.5640 - val_acc: 0.8020\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3648 - acc: 0.8712 - val_loss: 0.5654 - val_acc: 0.8027\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3632 - acc: 0.8718 - val_loss: 0.5637 - val_acc: 0.8037\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3617 - acc: 0.8726 - val_loss: 0.5648 - val_acc: 0.8017\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3602 - acc: 0.8729 - val_loss: 0.5649 - val_acc: 0.8017\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3584 - acc: 0.8733 - val_loss: 0.5662 - val_acc: 0.8020\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3570 - acc: 0.8744 - val_loss: 0.5649 - val_acc: 0.8007\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 0s 13us/step - loss: 0.3559 - acc: 0.8744 - val_loss: 0.5655 - val_acc: 0.8020\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3540 - acc: 0.8755 - val_loss: 0.5676 - val_acc: 0.8017\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3525 - acc: 0.8758 - val_loss: 0.5717 - val_acc: 0.7987\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3512 - acc: 0.8755 - val_loss: 0.5686 - val_acc: 0.8020\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3496 - acc: 0.8771 - val_loss: 0.5693 - val_acc: 0.8000\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3483 - acc: 0.8779 - val_loss: 0.5720 - val_acc: 0.8027\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3467 - acc: 0.8791 - val_loss: 0.5730 - val_acc: 0.8013\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3455 - acc: 0.8780 - val_loss: 0.5718 - val_acc: 0.7983\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3440 - acc: 0.8797 - val_loss: 0.5699 - val_acc: 0.7990\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3430 - acc: 0.8788 - val_loss: 0.5717 - val_acc: 0.8013\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3416 - acc: 0.8792 - val_loss: 0.5729 - val_acc: 0.8023\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3402 - acc: 0.8803 - val_loss: 0.5725 - val_acc: 0.8020\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3387 - acc: 0.8811 - val_loss: 0.5755 - val_acc: 0.7980\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3376 - acc: 0.8815 - val_loss: 0.5743 - val_acc: 0.8013\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 0s 13us/step - loss: 0.3362 - acc: 0.8818 - val_loss: 0.5754 - val_acc: 0.7997\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3351 - acc: 0.8823 - val_loss: 0.5757 - val_acc: 0.8030\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 0s 13us/step - loss: 0.3336 - acc: 0.8834 - val_loss: 0.5770 - val_acc: 0.8013\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 0s 13us/step - loss: 0.3324 - acc: 0.8843 - val_loss: 0.5790 - val_acc: 0.8017\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3309 - acc: 0.8838 - val_loss: 0.5791 - val_acc: 0.7990\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3299 - acc: 0.8845 - val_loss: 0.5789 - val_acc: 0.8017\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3286 - acc: 0.8842 - val_loss: 0.5801 - val_acc: 0.8013\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 0s 14us/step - loss: 0.3272 - acc: 0.8852 - val_loss: 0.5824 - val_acc: 0.8010\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 0s 13us/step - loss: 0.3260 - acc: 0.8856 - val_loss: 0.5830 - val_acc: 0.7993\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3251 - acc: 0.8861 - val_loss: 0.5832 - val_acc: 0.7993\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3241 - acc: 0.8866 - val_loss: 0.5842 - val_acc: 0.7987\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3226 - acc: 0.8877 - val_loss: 0.5821 - val_acc: 0.8000\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3212 - acc: 0.8879 - val_loss: 0.5864 - val_acc: 0.7953\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3203 - acc: 0.8877 - val_loss: 0.5879 - val_acc: 0.7990\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3194 - acc: 0.8880 - val_loss: 0.5855 - val_acc: 0.8023\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3180 - acc: 0.8889 - val_loss: 0.5900 - val_acc: 0.7987\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3165 - acc: 0.8904 - val_loss: 0.5908 - val_acc: 0.7973\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3155 - acc: 0.8896 - val_loss: 0.5912 - val_acc: 0.7990\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 0s 12us/step - loss: 0.3143 - acc: 0.8914 - val_loss: 0.5912 - val_acc: 0.7970\n"
     ]
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 20us/step\n",
      "4000/4000 [==============================] - 0s 20us/step\n"
     ]
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.31041816379807213, 0.8919393939393939]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5526078445911408, 0.812]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# __SOLUTION__ \n",
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, you were able to get a fairly similar validation accuracy of 89.67 (compared to 88.45 in obtained in the first model in this lab). Your test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, you not only built an initial deep-learning model, you then used a validation set to tune your model using various types of regularization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
